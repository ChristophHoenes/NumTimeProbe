{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "14fa6e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'score - (team_a + team_b)'"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_template():\n",
    "    return \"{col1} + {col2}\"\n",
    "col_name_t = 'score'\n",
    "f\"{col_name_t} - ({gen_template()})\".format(col1 = 'team_a', col2 = 'team_b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e36e29d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 5']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TapexTokenizer, BartForConditionalGeneration, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "#tokenizer = TapexTokenizer.from_pretrained(\"microsoft/tapex-base-finetuned-wtq\")\n",
    "#model = BartForConditionalGeneration.from_pretrained(\"microsoft/tapex-base-finetuned-wtq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neulab/omnitab-large-finetuned-wtq\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"neulab/omnitab-large-finetuned-wtq\")\n",
    "\n",
    "data = {\n",
    "    \"year\": [2001, 2002, 2003, 2004, 2005, 2006], #[1896, 1900, 1904, 2004, 2008, 2012],\n",
    "    \"city\": [\"athens\", \"paris\", \"st. louis\", \"athens\", \"beijing\", \"london\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "\n",
    "# tapex accepts uncased input since it is pre-trained on the uncased corpus\n",
    "#query = \"In which year did beijing host the Olympic Games?\"\n",
    "#query = \"In which year did Germany host the Olympic Games?\"\n",
    "#query = \"How manny years passed between the first and second time the Olympic Games where hosted in athens?\"\n",
    "query = \"How manny years passed between the Olympic Games in paris and the Olympic Games in london?\"\n",
    "encoding = tokenizer(table=table, query=query, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**encoding)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# [' 2008.0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2125df1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' das']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(query=\"just some text\", answer='das', return_tensors=\"pt\")\n",
    "outputs = model.generate(**encoding)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f1bfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 6 years']\n"
     ]
    }
   ],
   "source": [
    "query = \"how long did it take for the new york americans to win the national cup after 1936?\"\n",
    "data_raw = { \"header\": [ \"Year\", \"Division\", \"League\", \"Reg. Season\", \"Playoffs\", \"National Cup\" ], \"rows\": [ [ \"1931\", \"1\", \"ASL\", \"6th (Fall)\", \"No playoff\", \"N/A\" ], [ \"Spring 1932\", \"1\", \"ASL\", \"5th?\", \"No playoff\", \"1st Round\" ], [ \"Fall 1932\", \"1\", \"ASL\", \"3rd\", \"No playoff\", \"N/A\" ], [ \"Spring 1933\", \"1\", \"ASL\", \"?\", \"?\", \"Final\" ], [ \"1933/34\", \"N/A\", \"ASL\", \"2nd\", \"No playoff\", \"?\" ], [ \"1934/35\", \"N/A\", \"ASL\", \"2nd\", \"No playoff\", \"?\" ], [ \"1935/36\", \"N/A\", \"ASL\", \"1st\", \"Champion (no playoff)\", \"?\" ], [ \"1936/37\", \"N/A\", \"ASL\", \"5th, National\", \"Did not qualify\", \"Champion\" ], [ \"1937/38\", \"N/A\", \"ASL\", \"3rd(t), National\", \"1st Round\", \"?\" ], [ \"1938/39\", \"N/A\", \"ASL\", \"4th, National\", \"Did not qualify\", \"?\" ], [ \"1939/40\", \"N/A\", \"ASL\", \"4th\", \"No playoff\", \"?\" ], [ \"1940/41\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1941/42\", \"N/A\", \"ASL\", \"3rd\", \"No playoff\", \"?\" ], [ \"1942/43\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1943/44\", \"N/A\", \"ASL\", \"9th\", \"No playoff\", \"?\" ], [ \"1944/45\", \"N/A\", \"ASL\", \"9th\", \"No playoff\", \"?\" ], [ \"1945/46\", \"N/A\", \"ASL\", \"5th\", \"No playoff\", \"?\" ], [ \"1946/47\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1947/48\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1948/49\", \"N/A\", \"ASL\", \"1st(t)\", \"Finals\", \"?\" ], [ \"1949/50\", \"N/A\", \"ASL\", \"3rd\", \"No playoff\", \"?\" ], [ \"1950/51\", \"N/A\", \"ASL\", \"5th\", \"No playoff\", \"?\" ], [ \"1951/52\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1952/53\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"Semifinals\" ], [ \"1953/54\", \"N/A\", \"ASL\", \"1st\", \"Champion (no playoff)\", \"Champion\" ], [ \"1954/55\", \"N/A\", \"ASL\", \"8th\", \"No playoff\", \"?\" ], [ \"1955/56\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ] ], \"name\": \"csv/203-csv/435.tsv\" }\n",
    "data = {header: [row[header_id] for row in data_raw['rows']] for header_id, header in enumerate(data_raw['header'])}\n",
    "encoding = tokenizer(table=table, query=query, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**encoding)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ccc67d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT MAX( YEAR ) FROM track WHERE National Cup > 1936 AND Playoffs > 0 ;\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LarkAI/codet5p-770m_nl2sql_oig\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"LarkAI/codet5p-770m_nl2sql_oig\").to(device)\n",
    "\n",
    "data_raw = { \"header\": [ \"Year\", \"Division\", \"League\", \"Reg. Season\", \"Playoffs\", \"National Cup\" ], \"rows\": [ [ \"1931\", \"1\", \"ASL\", \"6th (Fall)\", \"No playoff\", \"N/A\" ], [ \"Spring 1932\", \"1\", \"ASL\", \"5th?\", \"No playoff\", \"1st Round\" ], [ \"Fall 1932\", \"1\", \"ASL\", \"3rd\", \"No playoff\", \"N/A\" ], [ \"Spring 1933\", \"1\", \"ASL\", \"?\", \"?\", \"Final\" ], [ \"1933/34\", \"N/A\", \"ASL\", \"2nd\", \"No playoff\", \"?\" ], [ \"1934/35\", \"N/A\", \"ASL\", \"2nd\", \"No playoff\", \"?\" ], [ \"1935/36\", \"N/A\", \"ASL\", \"1st\", \"Champion (no playoff)\", \"?\" ], [ \"1936/37\", \"N/A\", \"ASL\", \"5th, National\", \"Did not qualify\", \"Champion\" ], [ \"1937/38\", \"N/A\", \"ASL\", \"3rd(t), National\", \"1st Round\", \"?\" ], [ \"1938/39\", \"N/A\", \"ASL\", \"4th, National\", \"Did not qualify\", \"?\" ], [ \"1939/40\", \"N/A\", \"ASL\", \"4th\", \"No playoff\", \"?\" ], [ \"1940/41\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1941/42\", \"N/A\", \"ASL\", \"3rd\", \"No playoff\", \"?\" ], [ \"1942/43\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1943/44\", \"N/A\", \"ASL\", \"9th\", \"No playoff\", \"?\" ], [ \"1944/45\", \"N/A\", \"ASL\", \"9th\", \"No playoff\", \"?\" ], [ \"1945/46\", \"N/A\", \"ASL\", \"5th\", \"No playoff\", \"?\" ], [ \"1946/47\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1947/48\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1948/49\", \"N/A\", \"ASL\", \"1st(t)\", \"Finals\", \"?\" ], [ \"1949/50\", \"N/A\", \"ASL\", \"3rd\", \"No playoff\", \"?\" ], [ \"1950/51\", \"N/A\", \"ASL\", \"5th\", \"No playoff\", \"?\" ], [ \"1951/52\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1952/53\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"Semifinals\" ], [ \"1953/54\", \"N/A\", \"ASL\", \"1st\", \"Champion (no playoff)\", \"Champion\" ], [ \"1954/55\", \"N/A\", \"ASL\", \"8th\", \"No playoff\", \"?\" ], [ \"1955/56\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ] ], \"name\": \"csv/203-csv/435.tsv\" }\n",
    "text = f\"Given the following schema:\\ndf ({','.join(data_raw['header'])})\\nWrite a SQL query to determine how long it took the new york americans to win the national cup after 1936.\"\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(inputs, max_length=512)\n",
    "response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55150803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737d435f793e4b2aad9beede112cebed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efff8b705f14f8fa7ad784d092b6735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c28553069941ef8155cabd7d186208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ab5112dade475299ebdd792e25ba5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/35.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65972760047e4bd2bc1e078c98bb7109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fd2c00cbb649d1ac99c5e7c6de7905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00007.bin:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b734d98eae14a3580d457ee6bc93038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00007.bin:   0%|          | 0.00/9.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, CodeLlamaTokenizer\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefog/sqlcoder-34b-alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefog/sqlcoder-34b-alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m data_raw \u001b[38;5;241m=\u001b[39m { \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m: [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDivision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeague\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReg. Season\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlayoffs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNational Cup\u001b[39m\u001b[38;5;124m\"\u001b[39m ], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrows\u001b[39m\u001b[38;5;124m\"\u001b[39m: [ [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1931\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th (Fall)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpring 1932\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5th?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1st Round\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFall 1932\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3rd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpring 1933\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1933/34\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2nd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1934/35\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2nd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1935/36\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1st\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChampion (no playoff)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1936/37\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5th, National\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not qualify\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChampion\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1937/38\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3rd(t), National\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1st Round\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1938/39\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4th, National\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid not qualify\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1939/40\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1940/41\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1941/42\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3rd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1942/43\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1943/44\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1944/45\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m9th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1945/46\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1946/47\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1947/48\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1948/49\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1st(t)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinals\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1949/50\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3rd\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1950/51\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1951/52\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1952/53\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSemifinals\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1953/54\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1st\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChampion (no playoff)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChampion\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1954/55\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ], [ \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1955/56\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN/A\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m6th\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo playoff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m ] ], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv/203-csv/435.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m }\n\u001b[1;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven the following schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mdf (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_raw[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWrite a SQL query to determine how long it took the new york americans to win the national cup after 1936.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/transformers/modeling_utils.py:2864\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[1;32m   2863\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[0;32m-> 2864\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   2865\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2866\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2867\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   2868\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   2869\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   2870\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   2871\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2872\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   2873\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   2874\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   2875\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   2876\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   2877\u001b[0m     )\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;66;03m# load pt weights early so that we know which dtype to init the model under\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_pt:\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/transformers/utils/hub.py:1040\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[0;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m   1041\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   1042\u001b[0m             shard_filename,\n\u001b[1;32m   1043\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1044\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1045\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1046\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m   1047\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1048\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1049\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m   1050\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1051\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   1052\u001b[0m             _commit_hash\u001b[38;5;241m=\u001b[39m_commit_hash,\n\u001b[1;32m   1053\u001b[0m         )\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/transformers/utils/hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    430\u001b[0m         path_or_repo_id,\n\u001b[1;32m    431\u001b[0m         filename,\n\u001b[1;32m    432\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    433\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    434\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    435\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    436\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    437\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    438\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    439\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    440\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    441\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    442\u001b[0m     )\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/huggingface_hub/file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[1;32m   1362\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m-> 1364\u001b[0m     http_get(\n\u001b[1;32m   1365\u001b[0m         url_to_download,\n\u001b[1;32m   1366\u001b[0m         temp_file,\n\u001b[1;32m   1367\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1368\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[1;32m   1369\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1370\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(…)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(logger\u001b[38;5;241m.\u001b[39mgetEffectiveLevel() \u001b[38;5;241m==\u001b[39m logging\u001b[38;5;241m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, CodeLlamaTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"defog/sqlcoder-34b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"defog/sqlcoder-34b-alpha\")\n",
    "\n",
    "data_raw = { \"header\": [ \"Year\", \"Division\", \"League\", \"Reg. Season\", \"Playoffs\", \"National Cup\" ], \"rows\": [ [ \"1931\", \"1\", \"ASL\", \"6th (Fall)\", \"No playoff\", \"N/A\" ], [ \"Spring 1932\", \"1\", \"ASL\", \"5th?\", \"No playoff\", \"1st Round\" ], [ \"Fall 1932\", \"1\", \"ASL\", \"3rd\", \"No playoff\", \"N/A\" ], [ \"Spring 1933\", \"1\", \"ASL\", \"?\", \"?\", \"Final\" ], [ \"1933/34\", \"N/A\", \"ASL\", \"2nd\", \"No playoff\", \"?\" ], [ \"1934/35\", \"N/A\", \"ASL\", \"2nd\", \"No playoff\", \"?\" ], [ \"1935/36\", \"N/A\", \"ASL\", \"1st\", \"Champion (no playoff)\", \"?\" ], [ \"1936/37\", \"N/A\", \"ASL\", \"5th, National\", \"Did not qualify\", \"Champion\" ], [ \"1937/38\", \"N/A\", \"ASL\", \"3rd(t), National\", \"1st Round\", \"?\" ], [ \"1938/39\", \"N/A\", \"ASL\", \"4th, National\", \"Did not qualify\", \"?\" ], [ \"1939/40\", \"N/A\", \"ASL\", \"4th\", \"No playoff\", \"?\" ], [ \"1940/41\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1941/42\", \"N/A\", \"ASL\", \"3rd\", \"No playoff\", \"?\" ], [ \"1942/43\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1943/44\", \"N/A\", \"ASL\", \"9th\", \"No playoff\", \"?\" ], [ \"1944/45\", \"N/A\", \"ASL\", \"9th\", \"No playoff\", \"?\" ], [ \"1945/46\", \"N/A\", \"ASL\", \"5th\", \"No playoff\", \"?\" ], [ \"1946/47\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1947/48\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1948/49\", \"N/A\", \"ASL\", \"1st(t)\", \"Finals\", \"?\" ], [ \"1949/50\", \"N/A\", \"ASL\", \"3rd\", \"No playoff\", \"?\" ], [ \"1950/51\", \"N/A\", \"ASL\", \"5th\", \"No playoff\", \"?\" ], [ \"1951/52\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ], [ \"1952/53\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"Semifinals\" ], [ \"1953/54\", \"N/A\", \"ASL\", \"1st\", \"Champion (no playoff)\", \"Champion\" ], [ \"1954/55\", \"N/A\", \"ASL\", \"8th\", \"No playoff\", \"?\" ], [ \"1955/56\", \"N/A\", \"ASL\", \"6th\", \"No playoff\", \"?\" ] ], \"name\": \"csv/203-csv/435.tsv\" }\n",
    "text = f\"Given the following schema:\\ndf ({','.join(data_raw['header'])})\\nWrite a SQL query to determine how long it took the new york americans to win the national cup after 1936.\"\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(inputs, max_length=512)\n",
    "response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b76969b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL execution result:\n",
      "  MAX( \"YEAR\" )\n",
      "0   Spring 1933\n",
      "SELECT T1.year FROM df AS T1 JOIN df AS T2 ON T1.city = T2.city WHERE T2.year = 1980 INTERSECT SELECT T1.year FROM df AS T1 JOIN df AS T2 ON T1.city = T2.city WHERE T2.year = 1989 ;\n",
      "SQL execution result:\n",
      "Empty DataFrame\n",
      "Columns: [year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandasql import sqldf\n",
    "data_proc = {header: [row[h] for row in data_raw[\"rows\"]] for h, header in enumerate(data_raw[\"header\"])}\n",
    "df = pd.DataFrame.from_dict(data_proc)\n",
    "print('SQL execution result:')\n",
    "print(sqldf('SELECT MAX( \"YEAR\" ) FROM df WHERE \"National Cup\" > 1936 AND \"Playoffs\" > 0'))\n",
    "\n",
    "data = {\n",
    "    \"year\": [2001, 2002, 2003, 2004, 2005, 2006], #[1896, 1900, 1904, 2004, 2008, 2012],\n",
    "    \"city\": [\"athens\", \"paris\", \"st. louis\", \"athens\", \"beijing\", \"london\"]\n",
    "}\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "text = f\"Given the following schema:\\ndf ({','.join(['year', 'city'])})\\nWrite a SQL query to determine how manny years passed between the Olympic Games in paris and the Olympic Games in london.\"\n",
    "inputs = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(inputs, max_length=512)\n",
    "response_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(response_text)\n",
    "df = table\n",
    "print('SQL execution result:')\n",
    "print(sqldf(response_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "263ca49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadda\n"
     ]
    }
   ],
   "source": [
    "sqldf('SELECT * from df WHERE \"YEAR\" = \"\"')\n",
    "value = ''\n",
    "if value or True:\n",
    "    print('sadda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d2a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikitablequestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57dfac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'question', 'answers', 'table'],\n",
      "        num_rows: 11321\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'answers', 'table'],\n",
      "        num_rows: 4344\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'question', 'answers', 'table'],\n",
      "        num_rows: 2831\n",
      "    })\n",
      "})\n",
      "{'id': 'nu-0', 'question': 'which country had the most cyclists finish within the top 10?', 'answers': ['Italy'], 'table': {'header': ['Rank', 'Cyclist', 'Team', 'Time', 'UCI ProTour\\\\nPoints'], 'rows': [['1', 'Alejandro Valverde\\xa0(ESP)', \"Caisse d'Epargne\", '5h 29\\' 10\"', '40'], ['2', 'Alexandr Kolobnev\\xa0(RUS)', 'Team CSC Saxo Bank', 's.t.', '30'], ['3', 'Davide Rebellin\\xa0(ITA)', 'Gerolsteiner', 's.t.', '25'], ['4', 'Paolo Bettini\\xa0(ITA)', 'Quick Step', 's.t.', '20'], ['5', 'Franco Pellizotti\\xa0(ITA)', 'Liquigas', 's.t.', '15'], ['6', 'Denis Menchov\\xa0(RUS)', 'Rabobank', 's.t.', '11'], ['7', 'Samuel Sánchez\\xa0(ESP)', 'Euskaltel-Euskadi', 's.t.', '7'], ['8', 'Stéphane Goubert\\xa0(FRA)', 'Ag2r-La Mondiale', '+ 2\"', '5'], ['9', 'Haimar Zubeldia\\xa0(ESP)', 'Euskaltel-Euskadi', '+ 2\"', '3'], ['10', 'David Moncoutié\\xa0(FRA)', 'Cofidis', '+ 2\"', '1']], 'name': 'csv/203-csv/733.tsv'}}\n",
      "dict_keys(['id', 'question', 'answers', 'table'])\n",
      "dict_keys(['header', 'rows', 'name'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['test'][0])\n",
    "print(dataset['test'][0].keys())\n",
    "print(dataset['test'][0]['table'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "3e43b310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object: dict\n",
      "copy:shallow\n",
      "num_copies:100000\n",
      "time:\n",
      "0.11696150060743093\n",
      "\n",
      "object: dict\n",
      "copy:deep\n",
      "num_copies:100000\n",
      "time:\n",
      "0.21536239981651306\n",
      "\n",
      "object: nested dict\n",
      "copy:shallow\n",
      "num_copies:100000\n",
      "time:\n",
      "0.024984200485050678\n",
      "\n",
      "object: nested dict\n",
      "copy:deep\n",
      "num_copies:100000\n",
      "time:\n",
      "0.39652210008352995\n",
      "\n",
      "object: list\n",
      "copy:shallow\n",
      "num_copies:100000\n",
      "time:\n",
      "0.09884290024638176\n",
      "\n",
      "object: list\n",
      "copy:deep\n",
      "num_copies:100000\n",
      "time:\n",
      "2.242753100581467\n",
      "\n",
      "object: nested list\n",
      "copy:shallow\n",
      "num_copies:100000\n",
      "time:\n",
      "0.10539949964731932\n",
      "\n",
      "object: nested list\n",
      "copy:deep\n",
      "num_copies:100000\n",
      "time:\n",
      "167.24705380015075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from contexttimer import Timer\n",
    "\n",
    "iterations = 100000\n",
    "\n",
    "a = {\"data\": {'a':1,'b':2,'c':3}, \"description\": 'dict'}\n",
    "b = {\"data\": {'a':1,'b':2,'c':{'x':1.0, 'y':2.0, 'z':3.0}}, \"description\": 'nested dict'}\n",
    "c = {\"data\": list(range(100)), \"description\": 'list'}\n",
    "d = {\"data\": [list(range(i)) for i in range(100)], \"description\": 'nested list'}\n",
    "\n",
    "experiments = [(a, 'shallow'), (a, 'deep'), (b, 'shallow'), (b, 'deep'), (c, 'shallow'), (c, 'deep'), (d, 'shallow'), (d, 'deep')]\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    if experiment[1] == 'shallow':\n",
    "        copy_fn = copy.copy\n",
    "    elif experiment[1] == 'deep':\n",
    "        copy_fn = copy.deepcopy\n",
    "    else:\n",
    "        raise ValueError(f\"experiment type must be either 'shallow' or 'deep' but is '{experiment[1]}'!\")     \n",
    "        \n",
    "    print(f\"object: {experiment[0]['description']}\\ncopy:{experiment[1]}\\nnum_copies:{iterations}\\ntime:\")\n",
    "    with Timer() as t:\n",
    "            copies = [copy_fn(experiment[0]['data']) for i in range(iterations)]\n",
    "    print(t.elapsed)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "7eb2659e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    True\n",
       "1    True\n",
       "2    True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#set([1,1,2,2,3,3]) == set([1,2,3])\n",
    "pd.Series(list(set(pd.Series([1,1,2,2,3,3])))) == pd.Series([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "bbcee2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "416\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "regex = re.compile('\\d')\n",
    "#numeric_table_names_test = set([sample['table']['name'] for sample in dataset['test']]) # total tables 421\n",
    "#numeric_table_names_train = set([sample['table']['name'] for sample in dataset['train']]) # total tables 1333\n",
    "#numeric_table_names_val = set([sample['table']['name'] for sample in dataset['validation']]) # total tables 346\n",
    "#numeric_table_names = set([sample['table']['name'] for split in dataset.keys() for sample in dataset[split]]) # total tables 2100\n",
    "numeric_table_names_test = set([sample['table']['name'] \n",
    "                               # for split in dataset.keys() \n",
    "                               # for sample in dataset[split] \n",
    "                               for sample in dataset['test'] \n",
    "                               for col in sample['table']['rows'][0]\n",
    "                               if not re.search(regex, col) is None]\n",
    "                      )\n",
    "print(len(numeric_table_names_test))\n",
    "\n",
    "def deduplicate_column_names(column_names, extension_string: str = \"_\", use_numbering=True, while_killswitch: int = 3):\n",
    "    # TODO try finding patterns of column repetitions and assign them to leftmost (first column before the pattern) \n",
    "    # and concattenate names if they result in different pairs else try _1, _2 ,etc.\n",
    "    assert not (extension_string == \"\" and use_numbering is False), \"Either a non-empty extension_string or use_numbering=True must be used!\"\n",
    "    original_column_names = column_names\n",
    "    while_counter = 0\n",
    "    while len(set(column_names)) != len(column_names):\n",
    "        if while_counter > while_killswitch:\n",
    "            raise Exception(\n",
    "                f\"\"\"\n",
    "                Unusual depth of correlated/duplicated column names ({original_column_names}) detected!\n",
    "                Consider using different extension_string or a higher number of while_killswitch.\n",
    "                \"\"\"\n",
    "            )\n",
    "        col_name_counter = dict()\n",
    "        new_col_names = []\n",
    "        for col_name in column_names:\n",
    "            if col_name_counter.get(col_name) is None:\n",
    "                col_name_counter[col_name] = 1\n",
    "                new_col_names.append(col_name)\n",
    "            else:\n",
    "                col_name_counter[col_name] += 1\n",
    "                new_col_names.append(col_name + f\"{extension_string}{col_name_counter[col_name] if use_numbering else ''}\")\n",
    "        column_names = new_col_names\n",
    "        while_counter += 1\n",
    "    return column_names\n",
    "    \n",
    "\n",
    "# extract_tables = [(sample['table']['name'], sample['table']['header'], sample['table']['rows']) for sample in dataset['test'] if sample['table']['name'] in numeric_table_names_test]\n",
    "# print(len(extract_tables))\n",
    "dataselect = dict()\n",
    "original_format = dict()\n",
    "for target_name in numeric_table_names_test:\n",
    "    for sample in dataset['test']:\n",
    "        table_name = sample['table']['name']\n",
    "        if target_name != table_name:\n",
    "            continue\n",
    "        rows = dataselect.get(table_name)\n",
    "        if rows is None:\n",
    "            deduplicated_columns = deduplicate_column_names(sample['table']['header'])\n",
    "            dataselect[table_name] = pd.DataFrame.from_dict({i: row for i, row in enumerate(sample['table']['rows'])}, orient='index', columns=deduplicated_columns)\n",
    "            original_format[table_name] = sample['table']['rows']\n",
    "        # ensure that tables do not vary from question to question on the same table name (only do this once as computationally expensive) result = pass\n",
    "        # else:\n",
    "        #     assert sample['table']['rows'] == original_format[table_name], \"Differing rows for same table name detected! Should be equal for every question.\"\n",
    "    \n",
    "print(len(dataselect))\n",
    "#sample for sample in dataset['test'] if sample['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d10b2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideas for probing datasets\n",
    "# numeric only (exclude mixed) regex ({\\d})*{.}\\d*\n",
    "# 1. Create simple SQL like operators telling once the exact column names and operators and once more implicitly\n",
    "#    min,max,avg,sum,count + conditions(categorical column values, secondary numerical col constraint) between range (histogram), ratio between two numbers\n",
    "# 2. Comparison which is higher entity a or b in numeric column n? (ignore multi-dimensional)\n",
    "#    This also is similar to group by if there are multiple rows for the entity column (no primary key) -> combine with aggregators\n",
    "# 3. Take numeric subset of test set questions. Label all 4k examples if numeric extensive or not (exclude exact match task same as char)\n",
    "# 4. Infer meaning of multidimensional numerical columns (1-0-2) or hidden numeric information in texts by manually targeted questions\n",
    "# 5. One or multi column outlier detection ground truth is product of estimated probability via histogram of bin size x (same with all combinations of joint probability / concatenation of distinct values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a408709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SQL Templates\n",
    "#SELECT <operator><num_col></end> FROM <num_table> WHERE true <or> SELECT <num_col>/<num_col_2> FROM <num_table> WHERE true\n",
    "#<option>and <text_col> = <t_value></end>\n",
    "#<option>and <num_col> between <n_value> and <n_value></end>\n",
    "# Question translations:\n",
    "# a) literally:\n",
    "# What is the <operator> of column <num_col> where column <text_col> has value <t_value> and <num_col> is between <n_value> and <n_value>?\n",
    "# b) more natural free-text style:\n",
    "# What is the best team that played at most in <n_value> and at least in <n_value>? infer score = best infer team=opponent infer at least and at most = range\n",
    "\n",
    "\n",
    "# First preprocess all tables with multi-dimensional num columns find tables with regex \\d(char+\\d+)+ then discard the interleaved characters and make numberss each own column _dim1, _dim2 ...\n",
    "# Questions are domain dependent -> question asked by human annotator -> might be easier to create dummy tables inspired by those tables with multi-dim num -> enables training/fine-tune; retrieve result on processed table but input table always original\n",
    "# Examples multi dim: score 1-2, 1-2-3, minute scored playA 12' playB 45' 64', volume/area 12x35x24 (compute volume questions), composition 50 iron 50 copper, list of ids 12,45,32 (avg length of list question)\n",
    "# add id column (int or distinct topic related words) + distractor columns num or text random values sometimes categorical sometimes many valued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "53b77954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pandasql import sqldf\n",
    "\n",
    "\n",
    "class NumericIntensiveDataSet():\n",
    "    \n",
    "    def __init__(self, database_dict):\n",
    "        self.database_dict = database_dict\n",
    "        self.samples = [] \n",
    "        \n",
    "    def add(self, nl_question, sql_query, table_name, operator, condition_columns=[], condition_types=[], compute_answer=True):\n",
    "        self.samples.append(NumericIntensiveDataSample(nl_question, sql_query, table_name, operator, condition_columns, condition_types, compute_answer))\n",
    "        self.samples[-1]._set_parent(self)\n",
    "        \n",
    "    def subsample(sample_attribute: str, accepted_values: tuple):\n",
    "        new_subset = NumericIntensiveDataSet(self.database_dict)\n",
    "        new_subset.samples = [sample for sample in self.samples if getattr(sample, sample_attribute) in accepted_values]\n",
    "        return new_subset\n",
    "        \n",
    "    def get_database(self):\n",
    "        return self.database_dict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "\n",
    "class NumericIntensiveDataSample():\n",
    "    \n",
    "    def __init__(self, nl_question, sql_query, table_name, operator, condition_columns=[], condition_types=[], compute_answer=True):\n",
    "        self.parent_dataset = None\n",
    "        self.nl_question = nl_question\n",
    "        self.sql_query = sql_query\n",
    "        self.answer = query_dataframe(get_dataframe_from_database(self.parent_dataset.get_database(), table_name), sql_query) if compute_answer else None\n",
    "        self.table_name = table_name\n",
    "        self.operator = operator\n",
    "        self.num_conditions = len(condition_types)condition_types\n",
    "        self.condition_types = condition_types\n",
    "        self.condition_columns = condition_columns\n",
    "        \n",
    "    def compute_answer(self):\n",
    "        if self.parent_dataset is None:\n",
    "            raise RuntimeException(\"Can only compute answer, when assigned to a NumericIntensiveDataSet.\")\n",
    "        self.answer = query_dataframe(get_dataframe_from_database(self.parent_dataset.get_database(), self.table_name), self.sql_query)\n",
    "        \n",
    "    def _set_parent(self, parent_object):\n",
    "        self.parent_dataset = parent_object\n",
    "\n",
    "        \n",
    "def get_dataframe_from_database(database, table_name):\n",
    "    return database[table_name]\n",
    "\n",
    "        \n",
    "def query_dataframe(table, sql_query):\n",
    "    df = table\n",
    "    #print(sql_query)\n",
    "    #print(df.head(1))\n",
    "    query_result = sqldf(sql_query)\n",
    "    if len(query_result) > 1:\n",
    "        raise RuntimeException(\"Query result of dataframe returned multiple rows. Make sure to use only queries that result in a unique answer.\")\n",
    "    return query_result.iloc[0,0]\n",
    "\n",
    "\n",
    "class WTQTableInfo():\n",
    "    \n",
    "    def __init__(self, name, table):\n",
    "        self.name = name\n",
    "        self.col_names = list(table.columns)\n",
    "        self.num_cols = len(self.col_names)\n",
    "        self.num_rows = len(table)\n",
    "        self.size = (self.num_cols, self.num_rows)\n",
    "        self.numeric_col_ids = [col_id for col_id, col_name in enumerate(self.col_names) if is_numeric(table.iloc[:, col_id])]\n",
    "        self.text_col_ids = [col_id for col_id, col_name in enumerate(self.col_names) if not col_id in self.numeric_col_ids]\n",
    "        self.distinct_values = {col_id: table.iloc[:, col_id].unique() for col_id, col_name in enumerate(self.col_names)}\n",
    "        \n",
    "\n",
    "def generate_table_info(database):\n",
    "    table_info_list = []\n",
    "    database_list = list(database.items())\n",
    "    for table_input in database_list:\n",
    "        table_info_list.append(WTQTableInfo(table_input[0], table_input[1]))\n",
    "    return table_info_list\n",
    "    \n",
    "        \n",
    "        \n",
    "def is_numeric(col):\n",
    "    regex = re.compile('(\\d(,\\d{3})*|\\d+)?(\\.\\d+)?')\n",
    "    return col.map(lambda x: not re.fullmatch(regex, x) is None).all()\n",
    "                        \n",
    "\n",
    "def test_numeric():\n",
    "    test_df = pd.DataFrame.from_dict({'num_1': ['1.02', '.2', '1,000,444.543', '1,000', '192391230'],\n",
    "                                      'num_2': ['1.02', '.2', '1,000,444.543', '1,000', ''],\n",
    "                                      'text_1': ['1.02', '.2', '1,000,444.543', '1,000', '1.'],\n",
    "                                      'text_2': ['1.02', '.2', '1,000,444.543', '1,000', '1.0 cm'],\n",
    "                                      'text_3': ['1.02', '.2', '1,000,444.543', '1,000', '3-1'],\n",
    "                                      'text_4': ['1.02', '.2', '1,000,444.543', '1,000', 'nan']\n",
    "                                     })\n",
    "    return {col: is_numeric(test_df[col]) for col in test_df.columns}\n",
    "\n",
    "\n",
    "# TODO maybe add percentile\n",
    "def generate_sql_question_pairs(database,\n",
    "                                table_info, \n",
    "                                operators={'sql':['min', 'max', 'avg', 'sum', 'count', ''], \n",
    "                                           'text':['minimum', 'maximum', 'average', 'sum', 'count', 'value']},\n",
    "                                compute_answer=False\n",
    "                               ):\n",
    "    \"\"\" \n",
    "    inputs:\n",
    "    table_data: dict(keys=[table_names[list], table_size[tuple] numeric_cols[list], text_cols[list], distinct_values[dict(keys=[table_name,col_name], values=distinct values[list])]])\n",
    "    NOT TRUE ANYMORE:returns: dict(keys=table_names,\n",
    "                      values=dict (keys=[NL question, SQL query],\n",
    "                                   values=[list[str], list[str]]\n",
    "                                   )\n",
    "                     )\n",
    "    \"\"\"\n",
    "    numeric_intensive_dataset = NumericIntensiveDataSet(database)\n",
    "    for table in table_info:\n",
    "        table_name = table.name\n",
    "        num_rows = table.size[1]\n",
    "        for num_col_id in table.numeric_col_ids:\n",
    "            for op_id, operator in enumerate(operators['sql']):\n",
    "                if operator == '':\n",
    "                    continue\n",
    "                # TODO add group by / order by  option\n",
    "                # TODO probe column order position understanding e.g avg \"of 5th column\" / \"of column no. 5\"\n",
    "                column_name = table.col_names[num_col_id]\n",
    "                sql = \"SELECT \" + operator + f\"(\\\"{column_name}\\\") FROM df\\n\" + \"WHERE true\\n\" \n",
    "                nl_question = f\"What is the {operators['text'][op_id]} of column \\\"{column_name}\\\"\"\n",
    "                numeric_intensive_dataset.add(nl_question + \"?\", sql, table_name, operator, compute_answer=compute_answer)\n",
    "                # case with single text condition\n",
    "                for text_col_id in table.text_col_ids:\n",
    "                    num_distinct_values = len(table.distinct_values[text_col_id])\n",
    "                    for value in table.distinct_values[text_col_id]:\n",
    "                        if operator == '' and num_rows > num_distinct_values:\n",
    "                            continue\n",
    "                        # TODO think if extra label if condition column num_rows ~= num_distinct_values -> single-lookup all aggregates same\n",
    "                        # otherwise real aggregations might be underrepresented\n",
    "                        text_condition_col_name = table.col_names[text_col_id]\n",
    "                        value_safe = value.replace(\"'\", \"''\")\n",
    "                        condition_template_sql = \"\\t\" + f\"AND \\\"{text_condition_col_name}\\\" = '{value_safe}'\"\n",
    "                        condition_template_text = f\" where column \\\"{text_condition_col_name}\\\" has value '{value}'?\"\n",
    "                        numeric_intensive_dataset.add(nl_question + condition_template_text, sql + condition_template_sql, table_name, operator, condition_columns=[text_condition_col_name], condition_types=['text'], compute_answer=compute_answer)\n",
    "                # case with single numeric condition (if available)\n",
    "                # TODO find values to compare -> percentiles(may be easy if column is coded as histogram), percentiles + noise (only in one direction + or - depending on >< operator)\n",
    "                #for num_col_comp in num_cols:\n",
    "                #    # TODO decide if this constraint is desired pro avoids weird questions con can miss interesting questions like wht is the next highest number after 5, \n",
    "                #    #if num_col == num_col_comp:\n",
    "                #    #    continue\n",
    "                #    condition_template =\n",
    "                # case with range condition on num / date (if available) special case of previous where two conditions coincide to be < and > on same column\n",
    "                # (case with condition that requires subquery computation)\n",
    "                # cases with 2 and 3 conditions (all permutations?)\n",
    "    return numeric_intensive_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3df6dcac",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (919988198.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[256], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(test_string.replace(''''''', ''''''''))\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "test_string = \"ejndwedklwe '\"\n",
    "print(test_string.replace(''''''', ''''''''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d3e081e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_1': True,\n",
       " 'num_2': True,\n",
       " 'text_1': False,\n",
       " 'text_2': False,\n",
       " 'text_3': False,\n",
       " 'text_4': False}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_numeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "628e767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_info_database = generate_table_info(dataselect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "44b9ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/203-csv/480.tsv\n",
      "['Year', 'Name', 'Label', 'Hot Black Singles', 'Club Play Singles']\n",
      "7\n",
      "(5, 7)\n",
      "[0]\n",
      "[1, 2, 3, 4]\n",
      "key:0, num_distinct_values:5\n",
      "key:1, num_distinct_values:6\n",
      "key:2, num_distinct_values:5\n",
      "key:3, num_distinct_values:2\n",
      "key:4, num_distinct_values:2\n"
     ]
    }
   ],
   "source": [
    "table_info_test_sample = table_info_database[29]\n",
    "print(table_info_test_sample.name)\n",
    "print(table_info_test_sample.col_names)\n",
    "print(table_info_test_sample.num_rows)\n",
    "print(table_info_test_sample.size)\n",
    "print(table_info_test_sample.numeric_col_ids)\n",
    "print(table_info_test_sample.text_col_ids)\n",
    "for k, v in table_info_test_sample.distinct_values.items():\n",
    "    print(f\"key:{k}, num_distinct_values:{len(v[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f9598fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164620\n"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = generate_sql_question_pairs(dataselect, table_info_database)\n",
    "print(len(synthetic_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "2664a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the minimum of column GP where column \"Yds\" has value '329'?\n",
      "SELECT min(\"GP\") FROM df\n",
      "WHERE true\n",
      "\tAND \"Yds\" = '329'\n",
      "None\n",
      "csv/202-csv/64.tsv\n",
      "min\n",
      "['text']\n",
      "1\n",
      "['Yds']\n"
     ]
    }
   ],
   "source": [
    "sample = synthetic_dataset.samples[13350]\n",
    "print(sample.nl_question)\n",
    "print(sample.sql_query)\n",
    "print(sample.answer)\n",
    "print(sample.table_name)\n",
    "print(sample.operator)\n",
    "print(sample.num_conditions)\n",
    "print(sample.condition_types)\n",
    "print(sample.condition_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9f8219fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandasql import sqldf\n",
    "\n",
    "df = list(dataselect.values())[0]\n",
    "sqldf(\"select max(Round) from df\").iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "cfd7633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████| 15000/15000 [07:42<00:00, 32.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for sample in tqdm(synthetic_dataset.samples[:15000]):\n",
    "    sample.compute_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "adacfd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of None answers: 14381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[619, 620, 621, 622, 623]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#synthetic_dataset.database_dict = dataselect\n",
    "#synthetic_dataset.samples[100].parent_dataset.get_database()\n",
    "#get_dataframe_from_database(synthetic_dataset.samples[100].parent_dataset.get_database(), 'csv/203-csv/736.tsv')\n",
    "num_none_answers = sum([1 for sample in synthetic_dataset.samples[:15000] if sample.answer is None])\n",
    "none_answers = [sample_id for sample_id, sample in enumerate(synthetic_dataset.samples[:15000]) if sample.answer is None]\n",
    "print(f\"Number of None answers: {num_none_answers}\")\n",
    "none_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "6a59ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/203-csv/736.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Round</th>\n",
       "      <th>Date</th>\n",
       "      <th>Home/Away</th>\n",
       "      <th>Opponent team</th>\n",
       "      <th>Score</th>\n",
       "      <th>Scorers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>August 24, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Vllaznia Shkodër</td>\n",
       "      <td>0–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>August 30, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Teuta Durrës</td>\n",
       "      <td>0–1</td>\n",
       "      <td>Migen Memelli  42',</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>September 14, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KF Partizani Tirana</td>\n",
       "      <td>2–1</td>\n",
       "      <td>Daniel Xhafa  20', Andi Lila  90'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>September 20, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Apolonia Fier</td>\n",
       "      <td>0–2</td>\n",
       "      <td>Andi Lila  28', Migen Memelli  32'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>September 27, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Flamurtari Vlorë</td>\n",
       "      <td>3–1</td>\n",
       "      <td>Migen Memelli  17'  57'  61'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>October 4, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Bylis Ballsh</td>\n",
       "      <td>0–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>October 19, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Dinamo Tirana</td>\n",
       "      <td>2–1</td>\n",
       "      <td>Migen Memelli  32', Gjergji Muzaka  51'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>October 25, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Shkumbini Peqin</td>\n",
       "      <td>0–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>November 2, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Besa Kavaje</td>\n",
       "      <td>1–1</td>\n",
       "      <td>Jetmir Sefa  63',</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>November 9, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Elbasani</td>\n",
       "      <td>1–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>November 15, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Lushnja</td>\n",
       "      <td>1–0</td>\n",
       "      <td>Laert Ndoni (O.G)  13'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>November 23, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Vllaznia Shkoder</td>\n",
       "      <td>1–1</td>\n",
       "      <td>Bledar Devolli  79'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>November 29, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Teuta Durres</td>\n",
       "      <td>1–2</td>\n",
       "      <td>Jetmir Sefa  77'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>December 7, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KF Partizani Tirana</td>\n",
       "      <td>2–2</td>\n",
       "      <td>Jetmir Sefa  2', Migen Memelli  82'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>December 13, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Apolonia Fier</td>\n",
       "      <td>3–0</td>\n",
       "      <td>Daniel Xhafa  37'  48', Andi Lila  64'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>December 21, 2008</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Flamurtari Vlorë</td>\n",
       "      <td>2–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>December 27, 2008</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Bylis Ballsh</td>\n",
       "      <td>6–2</td>\n",
       "      <td>Pedro Neves (O.G)  2', Migen Memelli  4'  35' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>January 31, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Dinamo Tirana</td>\n",
       "      <td>2–4</td>\n",
       "      <td>Migen Memelli  8'  77', Ansi Agolli  58', Dani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>February 5, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Shkumbini Peqin</td>\n",
       "      <td>2–0</td>\n",
       "      <td>Andi Lila  27', Ansi Agolli  70'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>February 15, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Besa Kavaje</td>\n",
       "      <td>1–3</td>\n",
       "      <td>Daniel Xhafa  29', Ansi Agolli  53', Migen Mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>February 21, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Elbasani</td>\n",
       "      <td>0–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>March 1, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Lushnja</td>\n",
       "      <td>0–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>March 7, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Lushnja</td>\n",
       "      <td>2–1</td>\n",
       "      <td>Migen Memelli  22', Devis Mukaj  64'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>March 15, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Bylis Ballsh</td>\n",
       "      <td>0–0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>March 21, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>Apolonia Fier</td>\n",
       "      <td>2–0</td>\n",
       "      <td>Bledar Devolli  32', Devis Mukaj  75'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>April 5, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>Flamurtari Vlore</td>\n",
       "      <td>1–2</td>\n",
       "      <td>Migen Memelli  1'  37'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>April 11, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KF Elbasani</td>\n",
       "      <td>3–0</td>\n",
       "      <td>Daniel Xhafa  51', Devis Mukaj  77', Migen Mem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>April 18, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>Partizani Tirana</td>\n",
       "      <td>2–2</td>\n",
       "      <td>Daniel Xhafa  56'  87'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>April 25, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Teuta Durres</td>\n",
       "      <td>4–1</td>\n",
       "      <td>Migen Memelli  5'  74', Daniel Xhafa  41', Ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>May 2, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>KS Besa Kavaje</td>\n",
       "      <td>1–1</td>\n",
       "      <td>Migen Memelli  20'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>May 9, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Shkumbini Peqin</td>\n",
       "      <td>3–1</td>\n",
       "      <td>Ahmed Mujdragic (O.G)  18', Gjergji Muzaka  27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>May 16, 2009</td>\n",
       "      <td>Away</td>\n",
       "      <td>Dinamo Tirana</td>\n",
       "      <td>2–3</td>\n",
       "      <td>Ansi Agolli  12', Sabien Lila  40', Ergys Sorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>May 23, 2009</td>\n",
       "      <td>Home</td>\n",
       "      <td>KS Vllaznia Shkoder</td>\n",
       "      <td>2–1</td>\n",
       "      <td>Devis Mukaj  2', Migen Memelli  30'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Round                Date Home/Away        Opponent team Score  \\\n",
       "0      1     August 24, 2008      Away  KS Vllaznia Shkodër   0–0   \n",
       "1      2     August 30, 2008      Away      KS Teuta Durrës   0–1   \n",
       "2      3  September 14, 2008      Home  KF Partizani Tirana   2–1   \n",
       "3      4  September 20, 2008      Away     KS Apolonia Fier   0–2   \n",
       "4      5  September 27, 2008      Home  KS Flamurtari Vlorë   3–1   \n",
       "5      6     October 4, 2008      Away      KS Bylis Ballsh   0–0   \n",
       "6      7    October 19, 2008      Home     KS Dinamo Tirana   2–1   \n",
       "7      8    October 25, 2008      Away   KS Shkumbini Peqin   0–0   \n",
       "8      9    November 2, 2008      Home       KS Besa Kavaje   1–1   \n",
       "9     10    November 9, 2008      Away          KS Elbasani   1–0   \n",
       "10    11   November 15, 2008      Home           KS Lushnja   1–0   \n",
       "11    12   November 23, 2008      Home  KS Vllaznia Shkoder   1–1   \n",
       "12    13   November 29, 2008      Home      KS Teuta Durres   1–2   \n",
       "13    14    December 7, 2008      Away  KF Partizani Tirana   2–2   \n",
       "14    15   December 13, 2008      Home     KS Apolonia Fier   3–0   \n",
       "15    16   December 21, 2008      Away  KS Flamurtari Vlorë   2–0   \n",
       "16    17   December 27, 2008      Home      KS Bylis Ballsh   6–2   \n",
       "17    18    January 31, 2009      Away     KS Dinamo Tirana   2–4   \n",
       "18    19    February 5, 2009      Home   KS Shkumbini Peqin   2–0   \n",
       "19    20   February 15, 2009      Away       KS Besa Kavaje   1–3   \n",
       "20    21   February 21, 2009      Home          KS Elbasani   0–0   \n",
       "21    22       March 1, 2009      Away           KS Lushnja   0–0   \n",
       "22    23       March 7, 2009      Home           KS Lushnja   2–1   \n",
       "23    24      March 15, 2009      Away      KS Bylis Ballsh   0–0   \n",
       "24    25      March 21, 2009      Home        Apolonia Fier   2–0   \n",
       "25    26       April 5, 2009      Away     Flamurtari Vlore   1–2   \n",
       "26    27      April 11, 2009      Home          KF Elbasani   3–0   \n",
       "27    28      April 18, 2009      Away     Partizani Tirana   2–2   \n",
       "28    29      April 25, 2009      Home      KS Teuta Durres   4–1   \n",
       "29    30         May 2, 2009      Away       KS Besa Kavaje   1–1   \n",
       "30    31         May 9, 2009      Home   KS Shkumbini Peqin   3–1   \n",
       "31    32        May 16, 2009      Away        Dinamo Tirana   2–3   \n",
       "32    33        May 23, 2009      Home  KS Vllaznia Shkoder   2–1   \n",
       "\n",
       "                                              Scorers  \n",
       "0                                                      \n",
       "1                                 Migen Memelli  42',  \n",
       "2                   Daniel Xhafa  20', Andi Lila  90'  \n",
       "3                  Andi Lila  28', Migen Memelli  32'  \n",
       "4                        Migen Memelli  17'  57'  61'  \n",
       "5                                                      \n",
       "6             Migen Memelli  32', Gjergji Muzaka  51'  \n",
       "7                                                      \n",
       "8                                   Jetmir Sefa  63',  \n",
       "9                                                      \n",
       "10                             Laert Ndoni (O.G)  13'  \n",
       "11                                Bledar Devolli  79'  \n",
       "12                                   Jetmir Sefa  77'  \n",
       "13                Jetmir Sefa  2', Migen Memelli  82'  \n",
       "14             Daniel Xhafa  37'  48', Andi Lila  64'  \n",
       "15                                                     \n",
       "16  Pedro Neves (O.G)  2', Migen Memelli  4'  35' ...  \n",
       "17  Migen Memelli  8'  77', Ansi Agolli  58', Dani...  \n",
       "18                   Andi Lila  27', Ansi Agolli  70'  \n",
       "19  Daniel Xhafa  29', Ansi Agolli  53', Migen Mem...  \n",
       "20                                                     \n",
       "21                                                     \n",
       "22               Migen Memelli  22', Devis Mukaj  64'  \n",
       "23                                                     \n",
       "24              Bledar Devolli  32', Devis Mukaj  75'  \n",
       "25                             Migen Memelli  1'  37'  \n",
       "26  Daniel Xhafa  51', Devis Mukaj  77', Migen Mem...  \n",
       "27                             Daniel Xhafa  56'  87'  \n",
       "28  Migen Memelli  5'  74', Daniel Xhafa  41', Ans...  \n",
       "29                                 Migen Memelli  20'  \n",
       "30  Ahmed Mujdragic (O.G)  18', Gjergji Muzaka  27...  \n",
       "31  Ansi Agolli  12', Sabien Lila  40', Ergys Sorr...  \n",
       "32                Devis Mukaj  2', Migen Memelli  30'  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataselect_list = list(dataselect.items())\n",
    "print(dataselect_list[0][0])\n",
    "dataselect_list[0][1]\n",
    "\n",
    "# time questions\n",
    "# condition score on month, weekday, year, week of month\n",
    "\n",
    "# numeric questions\n",
    "# condition on Home/Away \n",
    "# condition on minute goals where scored Who scored the second goal in the game on date x?\n",
    "\n",
    "# difficult transfer \n",
    "# How many games are won (at home)? How many draws (with at least2 goals)?\n",
    "# Who scored the most goals? (not even answerable with SQL)\n",
    "# Against which team did they score the most goals?\n",
    "# Which team is the strongest competitor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "f8a754d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Team', 'GP', 'Att', 'Yds', 'Avg', 'Long', 'Rush TD', 'Rec',\n",
      "       'Yds_2', 'Avg_2', 'Long_2', 'Rec TD'],\n",
      "      dtype='object')\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Team</th>\n",
       "      <th>GP</th>\n",
       "      <th>Att</th>\n",
       "      <th>Yds</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Long</th>\n",
       "      <th>Rush TD</th>\n",
       "      <th>Rec</th>\n",
       "      <th>Yds_2</th>\n",
       "      <th>Avg_2</th>\n",
       "      <th>Long_2</th>\n",
       "      <th>Rec TD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1981</td>\n",
       "      <td>San Diego Chargers</td>\n",
       "      <td>14</td>\n",
       "      <td>109</td>\n",
       "      <td>525</td>\n",
       "      <td>4.8</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>329</td>\n",
       "      <td>7.2</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982</td>\n",
       "      <td>San Digeo Chargers</td>\n",
       "      <td>9</td>\n",
       "      <td>87</td>\n",
       "      <td>430</td>\n",
       "      <td>4.9</td>\n",
       "      <td>48</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>66</td>\n",
       "      <td>5.1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1983</td>\n",
       "      <td>San Diego Chargers</td>\n",
       "      <td>15</td>\n",
       "      <td>127</td>\n",
       "      <td>516</td>\n",
       "      <td>4.1</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>215</td>\n",
       "      <td>8.6</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1984</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>15</td>\n",
       "      <td>103</td>\n",
       "      <td>396</td>\n",
       "      <td>3.8</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>268</td>\n",
       "      <td>7.9</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1985</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>16</td>\n",
       "      <td>192</td>\n",
       "      <td>929</td>\n",
       "      <td>4.8</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>576</td>\n",
       "      <td>10.5</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1986</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>16</td>\n",
       "      <td>205</td>\n",
       "      <td>1,087</td>\n",
       "      <td>5.3</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>686</td>\n",
       "      <td>12.7</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1987</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>280</td>\n",
       "      <td>3.1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>272</td>\n",
       "      <td>12.4</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1988</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>931</td>\n",
       "      <td>5.1</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>287</td>\n",
       "      <td>9.9</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1989</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>16</td>\n",
       "      <td>221</td>\n",
       "      <td>1,239</td>\n",
       "      <td>5.6</td>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>306</td>\n",
       "      <td>8.3</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1990</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>16</td>\n",
       "      <td>195</td>\n",
       "      <td>1,004</td>\n",
       "      <td>5.1</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>269</td>\n",
       "      <td>10.3</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1991</td>\n",
       "      <td>Cincinnati Bengals</td>\n",
       "      <td>15</td>\n",
       "      <td>152</td>\n",
       "      <td>571</td>\n",
       "      <td>3.8</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>348</td>\n",
       "      <td>8.7</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1992</td>\n",
       "      <td>Tampa Bay Buccaneers</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1992</td>\n",
       "      <td>Cleveland Browns</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>2.9</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Career Totals</td>\n",
       "      <td></td>\n",
       "      <td>162</td>\n",
       "      <td>1,685</td>\n",
       "      <td>7,962</td>\n",
       "      <td>4.7</td>\n",
       "      <td>65</td>\n",
       "      <td>49</td>\n",
       "      <td>383</td>\n",
       "      <td>3,621</td>\n",
       "      <td>9.5</td>\n",
       "      <td>57</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Year                  Team   GP    Att    Yds  Avg Long Rush TD  \\\n",
       "0            1981    San Diego Chargers   14    109    525  4.8   28       3   \n",
       "1            1982    San Digeo Chargers    9     87    430  4.9   48       6   \n",
       "2            1983    San Diego Chargers   15    127    516  4.1   61       3   \n",
       "3            1984    Cincinnati Bengals   15    103    396  3.8   33       2   \n",
       "4            1985    Cincinnati Bengals   16    192    929  4.8   39       7   \n",
       "5            1986    Cincinnati Bengals   16    205  1,087  5.3   56       5   \n",
       "6            1987    Cincinnati Bengals    9     94    280  3.1   18       1   \n",
       "7            1988    Cincinnati Bengals   15    182    931  5.1   51       8   \n",
       "8            1989    Cincinnati Bengals   16    221  1,239  5.6   65       7   \n",
       "9            1990    Cincinnati Bengals   16    195  1,004  5.1   56       5   \n",
       "10           1991    Cincinnati Bengals   15    152    571  3.8   25       2   \n",
       "11           1992  Tampa Bay Buccaneers    2      5      6  1.2    4       0   \n",
       "12           1992      Cleveland Browns    4     13     38  2.9   13       0   \n",
       "13  Career Totals                        162  1,685  7,962  4.7   65      49   \n",
       "\n",
       "    Rec  Yds_2 Avg_2 Long_2 Rec TD  \n",
       "0    46    329   7.2     29      3  \n",
       "1    13     66   5.1     12      0  \n",
       "2    25    215   8.6     36      0  \n",
       "3    34    268   7.9     27      2  \n",
       "4    55    576  10.5     57      5  \n",
       "5    54    686  12.7     54      4  \n",
       "6    22    272  12.4     46      2  \n",
       "7    29    287   9.9     28      6  \n",
       "8    37    306   8.3     25      2  \n",
       "9    26    269  10.3     35      4  \n",
       "10   40    348   8.7     40      2  \n",
       "11    0      0   0.0      0      0  \n",
       "12    2     -1  -0.5      4      0  \n",
       "13  383  3,621   9.5     57     30  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_test_query = \"\"\"\n",
    "SELECT min(\"GP\") FROM df\n",
    "WHERE true\n",
    "    AND \"Yds_2\" = '329'\n",
    "\"\"\"\n",
    "for i in range(20,21):\n",
    "    print(dataselect_list[i][1].columns)\n",
    "    print(query_dataframe(dataselect_list[i][1], sql_test_query))\n",
    "dataselect_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "4671cc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' inborn him possible later care denying India across']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' philippines']"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "input_ids = torch.tensor([[11,5400,123,678,423,575, 12000, 666, 420]])\n",
    "tokenized = {'input_ids':input_ids, 'attention_mask':torch.ones_like(input_ids)}\n",
    "outputs = model.generate(**tokenized)\n",
    "print(tokenizer.batch_decode(input_ids, skip_special_tokens=True))\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "53599092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def table_batching(dataset):\n",
    "    table_dict = dict()\n",
    "    for sample in dataset:\n",
    "        if table_dict.get(sample.table_name) is None:\n",
    "            table_dict[sample.table_name] = {'table':get_dataframe_from_database(sample.parent_dataset.get_database(), sample.table_name),\n",
    "                                             'questions':[sample.nl_question],\n",
    "                                             'answers':[sample.answer]\n",
    "                                            }\n",
    "        else:\n",
    "            table_dict[sample.table_name]['questions'].append(sample.nl_question)\n",
    "            table_dict[sample.table_name]['answers'].append(sample.answer)\n",
    "    return table_dict\n",
    "\n",
    "table_batches = table_batching(synthetic_dataset.samples)\n",
    "len(table_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "00a96964",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyMatch:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def group(self, i: int):\n",
    "        return '-inf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "b1d99743",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_post_processing(predictions, strip_white=True, allow_punctuation=False, strip_unit=True, numeric=True, round_decimals=0):\n",
    "    processed = predictions\n",
    "    if strip_white:\n",
    "        processed = [item.strip() for item in predictions]\n",
    "    if numeric:\n",
    "        regex = re.compile('(\\d(,\\d{3})*|\\d+)?(\\.\\d+)?')\n",
    "        numbers_found = [round(float(re.search(regex, item).group(0).replace(',', '')), round_decimals) for item in processed]\n",
    "        return numbers_found\n",
    "    else:\n",
    "        if not allow_punctuation:\n",
    "            processed = [item.replace(',', '') for item in processed]\n",
    "        if strip_unit:\n",
    "            regex = re.compile('(\\d(,\\d{3})*|\\d+)?(\\.\\d+)?')\n",
    "            processed = [re.search(regex, item).group(0) for item in processed]\n",
    "        #processed = [item.split('.') for item in processed]\n",
    "        #for item in processed:\n",
    "        #    round_decimals\n",
    "    pass\n",
    "\n",
    "def answer_post_processing(predictions, round_decimals=0):\n",
    "    #regex = re.compile('((\\d(,\\d{3})*|\\d+)|\\d+)(\\.\\d+)?')\n",
    "    regex = re.compile(\"(\\d*)(\\.\\d*)?(\\d)\")\n",
    "    numbers_found = [round(float((re.search(regex, item.strip().replace(',', '')) or DummyMatch()).group(0)), round_decimals) for item in predictions]\n",
    "    return numbers_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "fc6ae2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '3', '555', '', '3']"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1.3.555..3'.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "f4491012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-inf'"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex = re.compile(\"(\\d*)(\\.\\d*)?(\\d)\")\n",
    "(re.search(regex, '  sfsf $') or DummyMatch()).group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "81a7da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▌                                                            | 1/100 [00:02<04:17,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█▏                                                           | 2/100 [00:05<04:12,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 24 == answer: 8?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▊                                                           | 3/100 [00:07<04:11,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 12 == answer: 12?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██▍                                                          | 4/100 [00:10<04:06,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|███                                                          | 5/100 [00:10<02:54,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1400 == answer: 1400?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|███▋                                                         | 6/100 [00:11<02:20,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 2009 == answer: 2009?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|████▎                                                        | 7/100 [00:12<01:55,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 26 == answer: 26?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|████▉                                                        | 8/100 [00:13<01:59,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 47.0 == answer: 47.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|█████▍                                                       | 9/100 [00:15<02:01,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 25.2 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|██████                                                      | 10/100 [00:16<02:02,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 29.9 == answer: 31.9?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|██████▌                                                     | 11/100 [00:18<02:02,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 19.6 == answer: 17.925?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|███████▏                                                    | 12/100 [00:19<02:03,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 6.67 == answer: 6.67?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|███████▊                                                    | 13/100 [00:21<02:07,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 52.8 == answer: 52.8?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|████████▍                                                   | 14/100 [00:22<02:08,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 4 == answer: 4?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█████████                                                   | 15/100 [00:24<02:10,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 5.0 == answer: 5.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|█████████▌                                                  | 16/100 [00:27<02:50,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 6 == answer: 100?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|██████████▏                                                 | 17/100 [00:30<03:16,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 40 == answer: 40?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|██████████▊                                                 | 18/100 [00:33<03:31,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 81 == answer: 131?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████▍                                                | 19/100 [00:37<03:45,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 152 == answer: 102?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████                                                | 20/100 [00:40<03:49,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 141 == answer: 353.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████████▌                                               | 21/100 [00:43<04:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 193.4 == answer: 802.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████████▏                                              | 22/100 [00:46<04:02,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 141 == answer: 117?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████████▊                                              | 23/100 [00:49<03:59,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 46 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████████▍                                             | 24/100 [00:53<03:57,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 141 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|███████████████                                             | 25/100 [00:56<03:56,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7 == answer: 4?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████████▌                                            | 26/100 [00:59<03:51,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0 == answer: 7?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|████████████████▏                                           | 27/100 [01:02<03:53,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 4 == answer: 8?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████████▊                                           | 28/100 [01:05<03:51,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 9 == answer: 8?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|█████████████████▍                                          | 29/100 [01:09<03:47,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 2 == answer: 2.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|██████████████████                                          | 30/100 [01:12<03:43,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0 == answer: 0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|██████████████████▌                                         | 31/100 [01:15<03:42,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 97 == answer: 15?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|███████████████████▏                                        | 32/100 [01:18<03:38,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 9 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███████████████████▊                                        | 33/100 [01:22<03:36,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|████████████████████▍                                       | 34/100 [01:25<03:34,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 102 == answer: 98?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|█████████████████████                                       | 35/100 [01:28<03:29,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 253 == answer: 253?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|█████████████████████▌                                      | 36/100 [01:31<03:26,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 143 == answer: 143?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|██████████████████████▏                                     | 37/100 [01:34<03:23,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 102 == answer: 202.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|██████████████████████▊                                     | 38/100 [01:38<03:18,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 102 == answer: 52.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|███████████████████████▍                                    | 39/100 [01:41<03:16,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1080 == answer: 806?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████████████                                    | 40/100 [01:44<03:15,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 551 == answer: 1338?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|████████████████████████▌                                   | 41/100 [01:47<03:09,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 10 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████████████▏                                  | 42/100 [01:48<02:22,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 4 == answer: ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|█████████████████████████▊                                  | 43/100 [01:49<01:48,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 3 == answer: 3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████████████▍                                 | 44/100 [01:52<02:08,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 40 == answer: 40?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|███████████████████████████                                 | 45/100 [01:55<02:19,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 9 == answer: 9.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████████████▌                                | 46/100 [01:58<02:27,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 31 == answer: 31?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████████████▏                               | 47/100 [02:01<02:30,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████████████▊                               | 48/100 [02:04<02:33,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 34.50 == answer: 34.50?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████████████▍                              | 49/100 [02:08<02:37,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 31.91 == answer: 9.5?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████                              | 50/100 [02:11<02:35,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 32.11 == answer: 64.33?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|██████████████████████████████▌                             | 51/100 [02:14<02:37,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 67.33 == answer: 6.93?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|███████████████████████████████▏                            | 52/100 [02:18<02:36,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 26.91 == answer: 18.6?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|███████████████████████████████▊                            | 53/100 [02:21<02:37,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 21.91 == answer: 24.2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|████████████████████████████████▍                           | 54/100 [02:25<02:33,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 41.91 == answer: 29.65?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████████████████████████████████                           | 55/100 [02:28<02:28,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 15.44 == answer: 15.44?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|█████████████████████████████████▌                          | 56/100 [02:31<02:25,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 141.44 == answer: 46.89?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|██████████████████████████████████▏                         | 57/100 [02:35<02:24,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|██████████████████████████████████▊                         | 58/100 [02:38<02:21,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 8 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|███████████████████████████████████▍                        | 59/100 [02:42<02:20,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 71.6 == answer: 71.6?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|████████████████████████████████████                        | 60/100 [02:45<02:18,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 71.6 == answer: 71.6?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|████████████████████████████████████▌                       | 61/100 [02:48<02:11,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 79.41 == answer: 80.5?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|█████████████████████████████████████▏                      | 62/100 [02:52<02:10,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 79.74 == answer: 55.15?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|█████████████████████████████████████▊                      | 63/100 [02:55<02:07,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 71.4 == answer: 76.06?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████████████████▍                     | 64/100 [02:59<02:04,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 61.15 == answer: 61.15?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████████████████                     | 65/100 [03:03<02:05,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 68.28 == answer: 78.52?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|███████████████████████████████████████▌                    | 66/100 [03:06<02:00,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 61.15 == answer: 68.52?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████████████████▏                   | 67/100 [03:10<01:55,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 71.6 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████████████████▊                   | 68/100 [03:13<01:50,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|█████████████████████████████████████████▍                  | 69/100 [03:17<01:50,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 924 == answer: 919?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████                  | 70/100 [03:21<01:49,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 949 == answer: 935?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████████████████▌                 | 71/100 [03:24<01:42,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 936 == answer: 936?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|███████████████████████████████████████████▏                | 72/100 [03:27<01:36,  3.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 924 == answer: 958?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████████████████▊                | 73/100 [03:30<01:30,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 936 == answer: 995?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|████████████████████████████████████████████▍               | 74/100 [03:34<01:27,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 941 == answer: 933.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████               | 75/100 [03:37<01:22,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 941.16 == answer: 933.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|█████████████████████████████████████████████▌              | 76/100 [03:40<01:19,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1747 == answer: 1914?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|██████████████████████████████████████████████▏             | 77/100 [03:44<01:15,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1747 == answer: 939?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|██████████████████████████████████████████████▊             | 78/100 [03:47<01:12,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 10 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|███████████████████████████████████████████████▍            | 79/100 [03:50<01:08,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 952 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████████████████████████████████████████████            | 80/100 [03:51<00:53,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 2 == answer: 2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|████████████████████████████████████████████████▌           | 81/100 [03:52<00:42,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 2 == answer: 2?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|█████████████████████████████████████████████████▏          | 82/100 [03:53<00:32,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0 == answer: 0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|█████████████████████████████████████████████████▊          | 83/100 [03:54<00:26,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 0 == answer: 0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|██████████████████████████████████████████████████▍         | 84/100 [03:55<00:21,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 3.0 == answer: 3.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|███████████████████████████████████████████████████         | 85/100 [03:56<00:18,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 4.5 == answer: 4.5?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|███████████████████████████████████████████████████▌        | 86/100 [03:57<00:16,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████████████████████████████████████████████████▏       | 87/100 [03:58<00:14,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|████████████████████████████████████████████████████▊       | 88/100 [03:59<00:12,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|█████████████████████████████████████████████████████▍      | 89/100 [04:01<00:13,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 2.24 == answer: 2.24?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████      | 90/100 [04:02<00:14,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 14 == answer: 14?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|██████████████████████████████████████████████████████▌     | 91/100 [04:04<00:13,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 9 == answer: 9?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|███████████████████████████████████████████████████████▏    | 92/100 [04:06<00:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 13 == answer: 13?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|███████████████████████████████████████████████████████▊    | 93/100 [04:08<00:11,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████████████████████▍   | 94/100 [04:10<00:10,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 571 == answer: 571.0?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████████████████████████████████████████████████████   | 95/100 [04:11<00:08,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 4.9 == answer: 4.9?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|█████████████████████████████████████████████████████████▌  | 96/100 [04:13<00:06,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 5 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|██████████████████████████████████████████████████████████▏ | 97/100 [04:15<00:05,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 28 == answer: 44.5?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|██████████████████████████████████████████████████████████▊ | 98/100 [04:16<00:03,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 3 == answer: 3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|███████████████████████████████████████████████████████████▍| 99/100 [04:18<00:01,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 1 == answer: 1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 100/100 [04:20<00:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 2 == answer: 2?\n",
      "0.2299999770000023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_preds = []\n",
    "ground_truth = []\n",
    "eval_list = []\n",
    "#batch_size=2\n",
    "for sample in tqdm(synthetic_dataset.samples[0:15000:150]):\n",
    "#for iteration in tqdm(range(len(synthetic_dataset.samples[:100])//batch_size)):\n",
    "    #batch = synthetic_dataset.samples[iteration*batch_size:iteration*batch_size+batch_size]\n",
    "# table batches on CPU take ages\n",
    "#for table_name in table_batches.keys(): \n",
    "    input_table=get_dataframe_from_database(sample.parent_dataset.get_database(), sample.table_name)\n",
    "    input_query=sample.nl_question\n",
    "    #input_table = table_batches[table_name]['table']\n",
    "    #input_query = table_batches[table_name]['questions']\n",
    "    tokenized = tokenizer(\n",
    "        table=input_table, \n",
    "        query=input_query,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    # prevent index out of range due to sequence length -> higher error eexpected for questions on later inputs\n",
    "    #tokenized['input_ids'] = tokenized['input_ids'][:, :1024]\n",
    "    #tokenized['attention_mask'] = tokenized['attention_mask'][:, :1024]\n",
    "    \n",
    "    outputs = model.generate(**tokenized)\n",
    "    text_answer = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    print(f\"pred: {text_answer[0].strip()} == answer: {sample.answer}?\")\n",
    "    raw_preds.append(text_answer[0])\n",
    "    ground_truth.append(str(sample.answer))\n",
    "    eval_list.append(sample.answer == text_answer[0].strip())\n",
    "\n",
    "print(sum(eval_list)/(len(eval_list)+0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "5c8a0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important properties to test\n",
    "# How are answers generated correct operation over different column?\n",
    "# How does performance compare with few other (numerical) columns vs. manny?\n",
    "# Sensitivity to formulation of question?\n",
    "# Truncation error or inability to understand? (tokenize only 400 tables with dummy question '' and check lengths that remain for question) identify tables that fit and compute performance separately\n",
    "# Effect of padding on performance?\n",
    "# Exact match (string vs. number representation) vs. distance metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "9ef2801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_proc_results = answer_post_processing(raw_preds, 2)\n",
    "#post_proc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "389e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_proc_labels = answer_post_processing(ground_truth, 2)\n",
    "#post_proc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "e721931d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48484848484848486"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def post_proc_acc(pred, labels):\n",
    "    logits = [1.0 if p == l else 0.0 for p, l in zip(pred, labels) if not (p == float('-inf') or l == float('-inf'))]\n",
    "    return sum(logits)/len(logits) if len(logits) > 0 else None, logits\n",
    "\n",
    "post_proc_acc(post_proc_results, post_proc_labels)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "8bf70274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 6,\n",
       " 8,\n",
       " 9,\n",
       " 24,\n",
       " 32,\n",
       " 37,\n",
       " 46,\n",
       " 62,\n",
       " 84,\n",
       " 94,\n",
       " 113,\n",
       " 116,\n",
       " 124,\n",
       " 126,\n",
       " 163,\n",
       " 168,\n",
       " 184,\n",
       " 195,\n",
       " 201,\n",
       " 210,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 217,\n",
       " 226,\n",
       " 235,\n",
       " 238,\n",
       " 251,\n",
       " 252,\n",
       " 270]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_token_lengths = dict()\n",
    "for table_name in table_batches.keys():\n",
    "    tokenized = tokenizer(\n",
    "        table=table_batches[table_name]['table'], \n",
    "        query='What is the maximum of column \\\"superlongcolumnname\\nblabla123\\\" where?',\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    table_token_lengths[table_name] = len(tokenized['input_ids'][0])\n",
    "    \n",
    "oversized_table_ids = [table_id for table_id, table_name in enumerate(table_token_lengths.keys()) if table_token_lengths[table_name] > 1024]\n",
    "print(len(oversized_table_ids))\n",
    "oversized_table_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "f2141e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 What is the minimum of column Round?\n",
      "96 What is the maximum of column Round?\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(synthetic_dataset.samples[:150]):\n",
    "    if len(x.condition_columns) != 0:\n",
    "        continue\n",
    "    print(i, x.nl_question)\n",
    "#print(f\"pred: {text_answer.strip()} == answer:{sample.answer}?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3a5acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/202-csv/258.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1980</th>\n",
       "      <th>1975</th>\n",
       "      <th>1975</th>\n",
       "      <th>1985</th>\n",
       "      <th>1985</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World</td>\n",
       "      <td>4,434,682,000</td>\n",
       "      <td>4,068,109,000</td>\n",
       "      <td>366,573,000</td>\n",
       "      <td>4,830,979,000</td>\n",
       "      <td>396,297,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa</td>\n",
       "      <td>469,618,000</td>\n",
       "      <td>408,160,000</td>\n",
       "      <td>61,458,000</td>\n",
       "      <td>541,814,000</td>\n",
       "      <td>72,196,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asia</td>\n",
       "      <td>2,632,335,000</td>\n",
       "      <td>2,397,512,000</td>\n",
       "      <td>234,823,000</td>\n",
       "      <td>2,887,552,000</td>\n",
       "      <td>255,217,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Europe</td>\n",
       "      <td>692,431,000</td>\n",
       "      <td>675,542,000</td>\n",
       "      <td>16,889,000</td>\n",
       "      <td>706,009,000</td>\n",
       "      <td>13,578,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Latin-America\\n&amp; Caribbean</td>\n",
       "      <td>361,401,000</td>\n",
       "      <td>321,906,000</td>\n",
       "      <td>39,495,000</td>\n",
       "      <td>401,469,000</td>\n",
       "      <td>40,068,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>North\\nAmerica</td>\n",
       "      <td>256,068,000</td>\n",
       "      <td>243,425,000</td>\n",
       "      <td>12,643,000</td>\n",
       "      <td>269,456,000</td>\n",
       "      <td>13,388,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>22,828,000</td>\n",
       "      <td>21,564,000</td>\n",
       "      <td>1,264,000</td>\n",
       "      <td>24,678,000</td>\n",
       "      <td>1,850,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        1980           1975         1975  \\\n",
       "0                       World  4,434,682,000  4,068,109,000  366,573,000   \n",
       "1                      Africa    469,618,000    408,160,000   61,458,000   \n",
       "2                        Asia  2,632,335,000  2,397,512,000  234,823,000   \n",
       "3                      Europe    692,431,000    675,542,000   16,889,000   \n",
       "4  Latin-America\\n& Caribbean    361,401,000    321,906,000   39,495,000   \n",
       "5              North\\nAmerica    256,068,000    243,425,000   12,643,000   \n",
       "6                     Oceania     22,828,000     21,564,000    1,264,000   \n",
       "\n",
       "            1985         1985  \n",
       "0  4,830,979,000  396,297,000  \n",
       "1    541,814,000   72,196,000  \n",
       "2  2,887,552,000  255,217,000  \n",
       "3    706,009,000   13,578,000  \n",
       "4    401,469,000   40,068,000  \n",
       "5    269,456,000   13,388,000  \n",
       "6     24,678,000    1,850,000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataselect_list[1][0])\n",
    "dataselect_list[1][1]\n",
    "\n",
    "# numeric questions\n",
    "# In 1980 what was the relative proportion of European population compared to the world population?\n",
    "# How many times was the population x greater or smaller than the population of y in year z?\n",
    "# What was the proportion of deaths in x relative to the World's/x's population?\n",
    "# What was the proportion of deaths in x relative x's deaths?\n",
    "# difference of numbers\n",
    "# sum of nuzmbers\n",
    "\n",
    "# hard general knowledge transfer\n",
    "# try south hemisphere vs north (where does asia belong)\n",
    "# try developed vs emerging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a024dcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/202-csv/58.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grp</th>\n",
       "      <th>Race Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>AJC Challenge Stakes</td>\n",
       "      <td>Open</td>\n",
       "      <td>Open</td>\n",
       "      <td>wfa</td>\n",
       "      <td>1000</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Surround Stakes</td>\n",
       "      <td>3YO</td>\n",
       "      <td>Fillies</td>\n",
       "      <td>sw</td>\n",
       "      <td>1400</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Warwick Stakes</td>\n",
       "      <td>Open</td>\n",
       "      <td>Open</td>\n",
       "      <td>wfa</td>\n",
       "      <td>1400</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Liverpool City Cup</td>\n",
       "      <td>Open</td>\n",
       "      <td>Open</td>\n",
       "      <td>qlty</td>\n",
       "      <td>1300</td>\n",
       "      <td>March</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Silver Shadow Stakes</td>\n",
       "      <td>3YO</td>\n",
       "      <td>Fillies</td>\n",
       "      <td>swp</td>\n",
       "      <td>1200</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Up And Coming Stakes</td>\n",
       "      <td>3YO</td>\n",
       "      <td>C&amp;G</td>\n",
       "      <td>swp</td>\n",
       "      <td>1200</td>\n",
       "      <td>August</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Grp             Race Name   Age      Sex Weight Distance    Date\n",
       "0   2  AJC Challenge Stakes  Open     Open    wfa     1000   March\n",
       "1   2       Surround Stakes   3YO  Fillies     sw     1400   March\n",
       "2   2        Warwick Stakes  Open     Open    wfa     1400  August\n",
       "3   3    Liverpool City Cup  Open     Open   qlty     1300   March\n",
       "4   3  Silver Shadow Stakes   3YO  Fillies    swp     1200  August\n",
       "5   3  Up And Coming Stakes   3YO      C&G    swp     1200  August"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataselect_list[2][0])\n",
    "dataselect_list[2][1]\n",
    "\n",
    "# date month only -> infer spring summer?\n",
    "\n",
    "# numeric\n",
    "# max + condition\n",
    "# avg + condition\n",
    "# Grp condition/discrete or numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f904cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/203-csv/738.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Common name</th>\n",
       "      <th>Binomial nomenclature</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Density ¹</th>\n",
       "      <th>Location</th>\n",
       "      <th>Characteristics, Usage and Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aini or Aangili</td>\n",
       "      <td>Artocarpus hirsutus</td>\n",
       "      <td>Yellowish brown</td>\n",
       "      <td>595 kg/m³</td>\n",
       "      <td>Maharashtra, Andhra Pradesh, Tamil Nadu, Karna...</td>\n",
       "      <td>Elastic, close-grained, and strong. It takes p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arjun</td>\n",
       "      <td>Terminalia arjuna Terminalia elliptica</td>\n",
       "      <td>Dark brown</td>\n",
       "      <td>870 kg/m³</td>\n",
       "      <td>Central India</td>\n",
       "      <td>It is heavy and strong. It has such uses as be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Axlewood</td>\n",
       "      <td>Anogeissus latifolia</td>\n",
       "      <td></td>\n",
       "      <td>930 kg/m³</td>\n",
       "      <td>Andhra Pradesh, Tamil Nadu, Maharashtra, Madhy...</td>\n",
       "      <td>It is very strong, hard and tough. It takes a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Babul</td>\n",
       "      <td>Acacia nilotica subsp. indica</td>\n",
       "      <td>Whitish red</td>\n",
       "      <td>835 kg/m³</td>\n",
       "      <td>Rajasthan, Andhra Pradesh, Maharashtra, Madhya...</td>\n",
       "      <td>It is strong, hard and tough and it takes up a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bakul</td>\n",
       "      <td>Mimusops elengi Mimusops parvifolia</td>\n",
       "      <td>Reddish brown</td>\n",
       "      <td>880 kg/m³</td>\n",
       "      <td>Some parts of North India</td>\n",
       "      <td>It is close-grained and tough. It is used for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bamboo</td>\n",
       "      <td>Family Poaceae, tribe Bambuseae</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Throughout India, especially Assam and Bengal</td>\n",
       "      <td>Not actually a tree, but a woody grass, it is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Banyan</td>\n",
       "      <td>Ficus benghalensis</td>\n",
       "      <td>Brown</td>\n",
       "      <td>580 kg/m³</td>\n",
       "      <td>Throughout India</td>\n",
       "      <td>It is strong and durable only under water. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Benteak</td>\n",
       "      <td>Lagerstoemia parviflora</td>\n",
       "      <td></td>\n",
       "      <td>675 kg/m³</td>\n",
       "      <td>Kerala, Madras, Maharashtra, Karnataka</td>\n",
       "      <td>It is strong and takes up a smooth surface. It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bijasal</td>\n",
       "      <td>Pterocarpus marsupium</td>\n",
       "      <td>Light brown</td>\n",
       "      <td>800 kg/m³</td>\n",
       "      <td>Karnataka, Andhra Pradesh, Madhya Pradesh, Mah...</td>\n",
       "      <td>It is coarse-grained, durable and strong but d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Casuarina</td>\n",
       "      <td>Casuarina spp.</td>\n",
       "      <td>Reddish brown</td>\n",
       "      <td>765 kg/m³</td>\n",
       "      <td>Andhra Pradesh, Tamil Nadu</td>\n",
       "      <td>It grows straight. It is strong and fibrous. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Coconut</td>\n",
       "      <td>Cocos nucifera</td>\n",
       "      <td>Reddish brown</td>\n",
       "      <td></td>\n",
       "      <td>Throughout coastal India</td>\n",
       "      <td>Takes polish. Requires preservative treatment....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Deodar</td>\n",
       "      <td>Cedrus deodara</td>\n",
       "      <td>Yellowish brown</td>\n",
       "      <td>560 kg/m³</td>\n",
       "      <td>Himalayas, Punjab, Uttar Pradesh</td>\n",
       "      <td>Deodar is the most important timber tree provi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Gambar</td>\n",
       "      <td>Gmelina arborea</td>\n",
       "      <td>Pale yellow</td>\n",
       "      <td>580 kg/m³</td>\n",
       "      <td>Central India, South India</td>\n",
       "      <td>It can be easily worked and is strong and dura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hopea</td>\n",
       "      <td>Hopea parviflora</td>\n",
       "      <td>Light to deep brown</td>\n",
       "      <td>1010 kg/m³</td>\n",
       "      <td>Madras, Kerala</td>\n",
       "      <td>Hopea is extremely strong and tough. It is dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Himalayan Elm, Indian Elm</td>\n",
       "      <td>Ulmus wallichiana</td>\n",
       "      <td>Red</td>\n",
       "      <td>960 kg/m³</td>\n",
       "      <td>Throughout India</td>\n",
       "      <td>It is moderately hard and strong. It is used f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ironwood, Penaga Lilin,\\nBosneak, Gangaw, Mesua</td>\n",
       "      <td>Mesua ferrea</td>\n",
       "      <td>Reddish brown</td>\n",
       "      <td>960–1060 kg/m³</td>\n",
       "      <td></td>\n",
       "      <td>Ironwood is durable though it is very hard and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Irul, Pyinkado</td>\n",
       "      <td>Xylia xylocarpa</td>\n",
       "      <td></td>\n",
       "      <td>830–1060 kg/m³</td>\n",
       "      <td>Karnataka, Kerala, Andhra Pradesh, Maharashtra...</td>\n",
       "      <td>It is very hard, heavy and durable. Difficult ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Mangifera caesia.</td>\n",
       "      <td>Yellow, darkens with age</td>\n",
       "      <td>595 kg/m³</td>\n",
       "      <td>Karnataka, Maharashtra, Tamil Nadu, Kerala</td>\n",
       "      <td>It is compact and even grained. It is moderate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jarul</td>\n",
       "      <td>Lagerstroemia flos-reginae</td>\n",
       "      <td>Light reddish gray</td>\n",
       "      <td>640 kg/m³</td>\n",
       "      <td>Assam, Bengal, Maharashtra</td>\n",
       "      <td>Hard and durable, it can be easily worked. It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Kathal, Keledang, Jackfruit</td>\n",
       "      <td>Artocarpus heterophyllus</td>\n",
       "      <td>Yellow to deep brown</td>\n",
       "      <td>800 kg/m³</td>\n",
       "      <td>Karnataka, Andhra Pradesh, Kerala, Maharashtra...</td>\n",
       "      <td>It is heavy and hard. It is durable under wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lauraceae, Saj</td>\n",
       "      <td>Lauraceae</td>\n",
       "      <td>Dark brown</td>\n",
       "      <td>880 kg/m³</td>\n",
       "      <td>Karnataka, Andhra Pradesh, Bihar, Orissa, Madh...</td>\n",
       "      <td>It is strong, hard and tough. It is subject to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Mahogany</td>\n",
       "      <td>Swietenia spp.</td>\n",
       "      <td>Reddish brown</td>\n",
       "      <td>720 kg/m³</td>\n",
       "      <td></td>\n",
       "      <td>It takes a good polish and is easily worked. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Mango</td>\n",
       "      <td>Mangifera spp</td>\n",
       "      <td>Deep gray</td>\n",
       "      <td>560–720 kg/m³</td>\n",
       "      <td>Throughout India</td>\n",
       "      <td>The mango tree is well known for its fruits. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mulberry</td>\n",
       "      <td>Morus spp.</td>\n",
       "      <td>Brown</td>\n",
       "      <td>650 kg/m³</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>It is strong, tough and elastic. It takes up a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Oak</td>\n",
       "      <td>Quercus spp.</td>\n",
       "      <td>Yellowish brown</td>\n",
       "      <td>865 kg/m³</td>\n",
       "      <td></td>\n",
       "      <td>Oak is strong and durable, with straight silve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Palm</td>\n",
       "      <td>Arecaceae</td>\n",
       "      <td>Dark brown</td>\n",
       "      <td>1040 kg/m³</td>\n",
       "      <td>Throughout India</td>\n",
       "      <td>It contains ripe wood in the outer crust. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Pine</td>\n",
       "      <td>Pinus spp.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Pine wood is hard and tough except white pine ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Red cedar</td>\n",
       "      <td></td>\n",
       "      <td>Red</td>\n",
       "      <td>480 kg/m³</td>\n",
       "      <td>Assam, Nagpur</td>\n",
       "      <td>It is soft and even grained. It is used for fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Rosewood</td>\n",
       "      <td>Dalbergia latifolia</td>\n",
       "      <td>Dark</td>\n",
       "      <td>850 kg/m³</td>\n",
       "      <td>Kerala, Karnataka, Maharashtra, Madhya Pradesh...</td>\n",
       "      <td>It is strong, tough and close-grained. It is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sal</td>\n",
       "      <td>Shorea robusta</td>\n",
       "      <td>Brown</td>\n",
       "      <td>880–1050 kg/m³</td>\n",
       "      <td>Karnataka, Andhra Pradesh, Maharashtra, Uttar ...</td>\n",
       "      <td>It is hard, fibrous and close-grained. It does...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Sandalwood</td>\n",
       "      <td>Santalum spp.</td>\n",
       "      <td>White or Red</td>\n",
       "      <td>930 kg/m³</td>\n",
       "      <td>Karnataka, Tamil Nadu, Kerala, Assam, Nagpur, ...</td>\n",
       "      <td>It has a pleasant smell. It is commonly used f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Satinwood</td>\n",
       "      <td>Chloroxylon swietenia</td>\n",
       "      <td>Yellow</td>\n",
       "      <td>960 kg/m³</td>\n",
       "      <td>Central and Southern India</td>\n",
       "      <td>It is very hard and durable. It is close grain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Simul</td>\n",
       "      <td>Bombax spp.</td>\n",
       "      <td>White</td>\n",
       "      <td>450 kg/m³</td>\n",
       "      <td>All over India</td>\n",
       "      <td>It is a loose grained, inferior quality wood. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Siris</td>\n",
       "      <td>Albizia spp.</td>\n",
       "      <td>Dark brown</td>\n",
       "      <td></td>\n",
       "      <td>North India</td>\n",
       "      <td>Hard and durable, Siris wood is difficult to w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Sissoo</td>\n",
       "      <td>Dalbergia sissoo</td>\n",
       "      <td>Dark brown</td>\n",
       "      <td>770 kg/m³</td>\n",
       "      <td>Mysore, Maharashtra, Assam, Bengal, Uttar Prad...</td>\n",
       "      <td>Also known as shisham or tali, this wood is st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Spruce</td>\n",
       "      <td>Picea spp.</td>\n",
       "      <td></td>\n",
       "      <td>480 kg/m³</td>\n",
       "      <td></td>\n",
       "      <td>Spruce wood resists decay and is not affected ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Sundri</td>\n",
       "      <td>Heritiera fomes</td>\n",
       "      <td>Dark red</td>\n",
       "      <td>960 kg/m³</td>\n",
       "      <td>Bengal</td>\n",
       "      <td>It is hard and tough. It is difficult to seaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Tamarind</td>\n",
       "      <td>Tamarindus indica</td>\n",
       "      <td>Dark brown</td>\n",
       "      <td>1280 kg/m³[citation needed]</td>\n",
       "      <td>All over India</td>\n",
       "      <td>Tamarind is knotty and durable. It is a beauti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Teak</td>\n",
       "      <td>Tectona grandis</td>\n",
       "      <td>Deep yellow to dark brown</td>\n",
       "      <td>639 kg/m³</td>\n",
       "      <td>Central India and Southern India</td>\n",
       "      <td>Moderately hard, teak is durable and fire-resi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Toon, Red Cedar</td>\n",
       "      <td>Toona ciliata</td>\n",
       "      <td>Reddish brown or dull red</td>\n",
       "      <td>450 kg/m³</td>\n",
       "      <td>Assam</td>\n",
       "      <td>It can be easily worked. It is light in weight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Common name  \\\n",
       "0                                   Aini or Aangili   \n",
       "1                                             Arjun   \n",
       "2                                          Axlewood   \n",
       "3                                             Babul   \n",
       "4                                             Bakul   \n",
       "5                                            Bamboo   \n",
       "6                                            Banyan   \n",
       "7                                           Benteak   \n",
       "8                                           Bijasal   \n",
       "9                                         Casuarina   \n",
       "10                                          Coconut   \n",
       "11                                           Deodar   \n",
       "12                                           Gambar   \n",
       "13                                            Hopea   \n",
       "14                        Himalayan Elm, Indian Elm   \n",
       "15  Ironwood, Penaga Lilin,\\nBosneak, Gangaw, Mesua   \n",
       "16                                   Irul, Pyinkado   \n",
       "17                                             Jack   \n",
       "18                                            Jarul   \n",
       "19                      Kathal, Keledang, Jackfruit   \n",
       "20                                   Lauraceae, Saj   \n",
       "21                                         Mahogany   \n",
       "22                                            Mango   \n",
       "23                                         Mulberry   \n",
       "24                                              Oak   \n",
       "25                                             Palm   \n",
       "26                                             Pine   \n",
       "27                                        Red cedar   \n",
       "28                                         Rosewood   \n",
       "29                                              Sal   \n",
       "30                                       Sandalwood   \n",
       "31                                        Satinwood   \n",
       "32                                            Simul   \n",
       "33                                            Siris   \n",
       "34                                           Sissoo   \n",
       "35                                           Spruce   \n",
       "36                                           Sundri   \n",
       "37                                         Tamarind   \n",
       "38                                             Teak   \n",
       "39                                  Toon, Red Cedar   \n",
       "\n",
       "                     Binomial nomenclature                     Colour  \\\n",
       "0                      Artocarpus hirsutus            Yellowish brown   \n",
       "1   Terminalia arjuna Terminalia elliptica                 Dark brown   \n",
       "2                     Anogeissus latifolia                              \n",
       "3            Acacia nilotica subsp. indica                Whitish red   \n",
       "4      Mimusops elengi Mimusops parvifolia              Reddish brown   \n",
       "5          Family Poaceae, tribe Bambuseae                              \n",
       "6                       Ficus benghalensis                      Brown   \n",
       "7                  Lagerstoemia parviflora                              \n",
       "8                    Pterocarpus marsupium                Light brown   \n",
       "9                           Casuarina spp.              Reddish brown   \n",
       "10                          Cocos nucifera              Reddish brown   \n",
       "11                          Cedrus deodara            Yellowish brown   \n",
       "12                         Gmelina arborea                Pale yellow   \n",
       "13                        Hopea parviflora        Light to deep brown   \n",
       "14                       Ulmus wallichiana                        Red   \n",
       "15                            Mesua ferrea              Reddish brown   \n",
       "16                         Xylia xylocarpa                              \n",
       "17                       Mangifera caesia.   Yellow, darkens with age   \n",
       "18              Lagerstroemia flos-reginae         Light reddish gray   \n",
       "19                Artocarpus heterophyllus       Yellow to deep brown   \n",
       "20                               Lauraceae                 Dark brown   \n",
       "21                          Swietenia spp.              Reddish brown   \n",
       "22                           Mangifera spp                  Deep gray   \n",
       "23                              Morus spp.                      Brown   \n",
       "24                            Quercus spp.            Yellowish brown   \n",
       "25                               Arecaceae                 Dark brown   \n",
       "26                              Pinus spp.                              \n",
       "27                                                                Red   \n",
       "28                     Dalbergia latifolia                       Dark   \n",
       "29                          Shorea robusta                      Brown   \n",
       "30                           Santalum spp.               White or Red   \n",
       "31                   Chloroxylon swietenia                     Yellow   \n",
       "32                             Bombax spp.                      White   \n",
       "33                            Albizia spp.                 Dark brown   \n",
       "34                        Dalbergia sissoo                 Dark brown   \n",
       "35                              Picea spp.                              \n",
       "36                         Heritiera fomes                   Dark red   \n",
       "37                       Tamarindus indica                 Dark brown   \n",
       "38                         Tectona grandis  Deep yellow to dark brown   \n",
       "39                           Toona ciliata  Reddish brown or dull red   \n",
       "\n",
       "                      Density ¹  \\\n",
       "0                     595 kg/m³   \n",
       "1                     870 kg/m³   \n",
       "2                     930 kg/m³   \n",
       "3                     835 kg/m³   \n",
       "4                     880 kg/m³   \n",
       "5                                 \n",
       "6                     580 kg/m³   \n",
       "7                     675 kg/m³   \n",
       "8                     800 kg/m³   \n",
       "9                     765 kg/m³   \n",
       "10                                \n",
       "11                    560 kg/m³   \n",
       "12                    580 kg/m³   \n",
       "13                   1010 kg/m³   \n",
       "14                    960 kg/m³   \n",
       "15               960–1060 kg/m³   \n",
       "16               830–1060 kg/m³   \n",
       "17                    595 kg/m³   \n",
       "18                    640 kg/m³   \n",
       "19                    800 kg/m³   \n",
       "20                    880 kg/m³   \n",
       "21                    720 kg/m³   \n",
       "22                560–720 kg/m³   \n",
       "23                    650 kg/m³   \n",
       "24                    865 kg/m³   \n",
       "25                   1040 kg/m³   \n",
       "26                                \n",
       "27                    480 kg/m³   \n",
       "28                    850 kg/m³   \n",
       "29               880–1050 kg/m³   \n",
       "30                    930 kg/m³   \n",
       "31                    960 kg/m³   \n",
       "32                    450 kg/m³   \n",
       "33                                \n",
       "34                    770 kg/m³   \n",
       "35                    480 kg/m³   \n",
       "36                    960 kg/m³   \n",
       "37  1280 kg/m³[citation needed]   \n",
       "38                    639 kg/m³   \n",
       "39                    450 kg/m³   \n",
       "\n",
       "                                             Location  \\\n",
       "0   Maharashtra, Andhra Pradesh, Tamil Nadu, Karna...   \n",
       "1                                       Central India   \n",
       "2   Andhra Pradesh, Tamil Nadu, Maharashtra, Madhy...   \n",
       "3   Rajasthan, Andhra Pradesh, Maharashtra, Madhya...   \n",
       "4                           Some parts of North India   \n",
       "5       Throughout India, especially Assam and Bengal   \n",
       "6                                    Throughout India   \n",
       "7              Kerala, Madras, Maharashtra, Karnataka   \n",
       "8   Karnataka, Andhra Pradesh, Madhya Pradesh, Mah...   \n",
       "9                          Andhra Pradesh, Tamil Nadu   \n",
       "10                           Throughout coastal India   \n",
       "11                   Himalayas, Punjab, Uttar Pradesh   \n",
       "12                         Central India, South India   \n",
       "13                                     Madras, Kerala   \n",
       "14                                   Throughout India   \n",
       "15                                                      \n",
       "16  Karnataka, Kerala, Andhra Pradesh, Maharashtra...   \n",
       "17         Karnataka, Maharashtra, Tamil Nadu, Kerala   \n",
       "18                         Assam, Bengal, Maharashtra   \n",
       "19  Karnataka, Andhra Pradesh, Kerala, Maharashtra...   \n",
       "20  Karnataka, Andhra Pradesh, Bihar, Orissa, Madh...   \n",
       "21                                                      \n",
       "22                                   Throughout India   \n",
       "23                                             Punjab   \n",
       "24                                                      \n",
       "25                                   Throughout India   \n",
       "26                                                      \n",
       "27                                      Assam, Nagpur   \n",
       "28  Kerala, Karnataka, Maharashtra, Madhya Pradesh...   \n",
       "29  Karnataka, Andhra Pradesh, Maharashtra, Uttar ...   \n",
       "30  Karnataka, Tamil Nadu, Kerala, Assam, Nagpur, ...   \n",
       "31                         Central and Southern India   \n",
       "32                                     All over India   \n",
       "33                                        North India   \n",
       "34  Mysore, Maharashtra, Assam, Bengal, Uttar Prad...   \n",
       "35                                                      \n",
       "36                                             Bengal   \n",
       "37                                     All over India   \n",
       "38                   Central India and Southern India   \n",
       "39                                              Assam   \n",
       "\n",
       "                    Characteristics, Usage and Status  \n",
       "0   Elastic, close-grained, and strong. It takes p...  \n",
       "1   It is heavy and strong. It has such uses as be...  \n",
       "2   It is very strong, hard and tough. It takes a ...  \n",
       "3   It is strong, hard and tough and it takes up a...  \n",
       "4   It is close-grained and tough. It is used for ...  \n",
       "5   Not actually a tree, but a woody grass, it is ...  \n",
       "6   It is strong and durable only under water. The...  \n",
       "7   It is strong and takes up a smooth surface. It...  \n",
       "8   It is coarse-grained, durable and strong but d...  \n",
       "9   It grows straight. It is strong and fibrous. I...  \n",
       "10  Takes polish. Requires preservative treatment....  \n",
       "11  Deodar is the most important timber tree provi...  \n",
       "12  It can be easily worked and is strong and dura...  \n",
       "13  Hopea is extremely strong and tough. It is dif...  \n",
       "14  It is moderately hard and strong. It is used f...  \n",
       "15  Ironwood is durable though it is very hard and...  \n",
       "16  It is very hard, heavy and durable. Difficult ...  \n",
       "17  It is compact and even grained. It is moderate...  \n",
       "18  Hard and durable, it can be easily worked. It ...  \n",
       "19  It is heavy and hard. It is durable under wate...  \n",
       "20  It is strong, hard and tough. It is subject to...  \n",
       "21  It takes a good polish and is easily worked. I...  \n",
       "22  The mango tree is well known for its fruits. I...  \n",
       "23  It is strong, tough and elastic. It takes up a...  \n",
       "24  Oak is strong and durable, with straight silve...  \n",
       "25  It contains ripe wood in the outer crust. The ...  \n",
       "26  Pine wood is hard and tough except white pine ...  \n",
       "27  It is soft and even grained. It is used for fu...  \n",
       "28  It is strong, tough and close-grained. It is a...  \n",
       "29  It is hard, fibrous and close-grained. It does...  \n",
       "30  It has a pleasant smell. It is commonly used f...  \n",
       "31  It is very hard and durable. It is close grain...  \n",
       "32  It is a loose grained, inferior quality wood. ...  \n",
       "33  Hard and durable, Siris wood is difficult to w...  \n",
       "34  Also known as shisham or tali, this wood is st...  \n",
       "35  Spruce wood resists decay and is not affected ...  \n",
       "36  It is hard and tough. It is difficult to seaso...  \n",
       "37  Tamarind is knotty and durable. It is a beauti...  \n",
       "38  Moderately hard, teak is durable and fire-resi...  \n",
       "39  It can be easily worked. It is light in weight...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataselect_list[3][0])\n",
    "dataselect_list[3][1]\n",
    "\n",
    "# numeric only density with unit\n",
    "# relative\n",
    "# min/max/mean + condition on location or color \n",
    "# lookup\n",
    "\n",
    "# transfer\n",
    "# Which/how many have a range as density (no certain density)?\n",
    "# different answers based on wording nuances of colors e.g. brown vs. brownish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "83f3b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what year did she receive her most recent highest rated position?\n",
      "how many competitions did slovakia place at least 2nd?\n",
      "how many times did the competitor compete in the european championships?\n",
      "which competitions did she compete in before the 2001 world championships?\n",
      "did she throw more meters in 2009 or 2008?\n",
      "what is the only year she ranked 13th?\n",
      "what was the best position the competitor achieved before 2002?\n",
      "how many more meters are listed for the 2002 world junior championships than the 2002 european championships?\n",
      "2002 and what other year did she rank 26th?\n",
      "where is the last venue the competitor played in 2009?\n",
      "what is the total number of 2nd place positions?\n",
      "what is the least entry listed under notes?\n",
      "csv/203-csv/819.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Competition</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Position</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999</td>\n",
       "      <td>World Youth Championships</td>\n",
       "      <td>Bydgoszcz, Poland</td>\n",
       "      <td>12th</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>World Junior Championships</td>\n",
       "      <td>Santiago, Chile</td>\n",
       "      <td>5th</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001</td>\n",
       "      <td>European Junior Championships</td>\n",
       "      <td>Grosseto, Italy</td>\n",
       "      <td>2nd</td>\n",
       "      <td>61.97 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001</td>\n",
       "      <td>World Championships</td>\n",
       "      <td>Edmonton, Canada</td>\n",
       "      <td>23rd</td>\n",
       "      <td>61.26 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>World Junior Championships</td>\n",
       "      <td>Kingston, Jamaica</td>\n",
       "      <td>2nd</td>\n",
       "      <td>63.91 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2002</td>\n",
       "      <td>European Championships</td>\n",
       "      <td>Munich, Germany</td>\n",
       "      <td>26th</td>\n",
       "      <td>60.28 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006</td>\n",
       "      <td>European Championships</td>\n",
       "      <td>Gothenburg, Sweden</td>\n",
       "      <td>26th</td>\n",
       "      <td>62.39 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2007</td>\n",
       "      <td>World Student Games</td>\n",
       "      <td>Bangkok, Thailand</td>\n",
       "      <td>5th</td>\n",
       "      <td>64.95 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2007</td>\n",
       "      <td>World Championships</td>\n",
       "      <td>Osaka, Japan</td>\n",
       "      <td>13th</td>\n",
       "      <td>68.15 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008</td>\n",
       "      <td>Olympic Games</td>\n",
       "      <td>Beijing, PR China</td>\n",
       "      <td>8th</td>\n",
       "      <td>71.00 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008</td>\n",
       "      <td>World Athletics Final</td>\n",
       "      <td>Stuttgart, Germany</td>\n",
       "      <td>2nd</td>\n",
       "      <td>71.40 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2009</td>\n",
       "      <td>World Student Games</td>\n",
       "      <td>Belgrade, Serbia</td>\n",
       "      <td>2nd</td>\n",
       "      <td>72.85 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009</td>\n",
       "      <td>World Championships</td>\n",
       "      <td>Berlin, Germany</td>\n",
       "      <td>3rd</td>\n",
       "      <td>74.79 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009</td>\n",
       "      <td>World Athletics Final</td>\n",
       "      <td>Thessaloniki, Greece</td>\n",
       "      <td>3rd</td>\n",
       "      <td>70.45 m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2012</td>\n",
       "      <td>European Championships</td>\n",
       "      <td>Helsinki, Finland</td>\n",
       "      <td>2nd</td>\n",
       "      <td>73.34 m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year                    Competition                 Venue Position  \\\n",
       "0   1999      World Youth Championships     Bydgoszcz, Poland     12th   \n",
       "1   2000     World Junior Championships       Santiago, Chile      5th   \n",
       "2   2001  European Junior Championships       Grosseto, Italy      2nd   \n",
       "3   2001            World Championships      Edmonton, Canada     23rd   \n",
       "4   2002     World Junior Championships     Kingston, Jamaica      2nd   \n",
       "5   2002         European Championships       Munich, Germany     26th   \n",
       "6   2006         European Championships    Gothenburg, Sweden     26th   \n",
       "7   2007            World Student Games     Bangkok, Thailand      5th   \n",
       "8   2007            World Championships          Osaka, Japan     13th   \n",
       "9   2008                  Olympic Games     Beijing, PR China      8th   \n",
       "10  2008          World Athletics Final    Stuttgart, Germany      2nd   \n",
       "11  2009            World Student Games      Belgrade, Serbia      2nd   \n",
       "12  2009            World Championships       Berlin, Germany      3rd   \n",
       "13  2009          World Athletics Final  Thessaloniki, Greece      3rd   \n",
       "14  2012         European Championships     Helsinki, Finland      2nd   \n",
       "\n",
       "      Notes  \n",
       "0            \n",
       "1            \n",
       "2   61.97 m  \n",
       "3   61.26 m  \n",
       "4   63.91 m  \n",
       "5   60.28 m  \n",
       "6   62.39 m  \n",
       "7   64.95 m  \n",
       "8   68.15 m  \n",
       "9   71.00 m  \n",
       "10  71.40 m  \n",
       "11  72.85 m  \n",
       "12  74.79 m  \n",
       "13  70.45 m  \n",
       "14  73.34 m  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in dataset['test']:\n",
    "    table_name = sample['table']['name']\n",
    "    if table_name == 'csv/203-csv/819.tsv':\n",
    "        print(sample['question'])\n",
    "\n",
    "print(dataselect_list[4][0])\n",
    "dataselect_list[4][1]\n",
    "\n",
    "#numeric\n",
    "# What was the average distance with which the athlet could achieve second place in a world championship?\n",
    "# Is x meters a good distance for achieving at least xth place in a <senior/youth> <european/world> championship?\n",
    "\n",
    "# time/numeric questions\n",
    "# What was the longest period without tournaments?\n",
    "\n",
    "# general\n",
    "# Which years had the most competitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cae39925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who was the first and only member of team europe to beat team usa during the evening session of the third day of the 2007 weber cup?\n",
      "what total number of matches did team usa win after match no. 27?\n",
      "how many matches did team europe win?\n",
      "which player from team usa played in the first match listed?\n",
      "how many matches had a tie for the progressive total?\n",
      "which match no. has the lowest combined score?\n",
      "was paul moor in a singles match after team europe?\n",
      "how many total points did chris barnes score during the day 3 evening session of the 2007 weber cup?\n",
      "was chris barnes is more matches than doug kent?\n",
      "how many match no. have a combined score above 450?\n",
      "what was the number of players listed for each team?\n",
      "from team europe, who was the player to play in the match after mika koivuniemi?\n",
      "how many singles matches was tore torgerson in during the evening session on day 3?\n",
      "csv/203-csv/380.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match no.</th>\n",
       "      <th>Match Type</th>\n",
       "      <th>Team Europe</th>\n",
       "      <th>Score</th>\n",
       "      <th>Team USA</th>\n",
       "      <th>Progressive Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Mika Koivuniemi</td>\n",
       "      <td>217 - 279</td>\n",
       "      <td>Chris Barnes</td>\n",
       "      <td>14 - 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Osku Palermaa</td>\n",
       "      <td>217 - 244</td>\n",
       "      <td>Tommy Jones</td>\n",
       "      <td>14 - 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Paul Moor</td>\n",
       "      <td>210 - 199</td>\n",
       "      <td>Tim Mack</td>\n",
       "      <td>15 - 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Tore Torgersen</td>\n",
       "      <td>206 - 275</td>\n",
       "      <td>Doug Kent</td>\n",
       "      <td>15 - 13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Tomas Leandersson</td>\n",
       "      <td>176 - 258</td>\n",
       "      <td>Bill Hoffman</td>\n",
       "      <td>15 - 14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>Baker</td>\n",
       "      <td>Team Europe</td>\n",
       "      <td>202 - 203</td>\n",
       "      <td>Team USA</td>\n",
       "      <td>15 - 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Tore Torgersen</td>\n",
       "      <td>202 - 264</td>\n",
       "      <td>Chris Barnes</td>\n",
       "      <td>15 - 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32</td>\n",
       "      <td>Singles</td>\n",
       "      <td>Osku Palermaa</td>\n",
       "      <td>196 - 235</td>\n",
       "      <td>Tommy Jones</td>\n",
       "      <td>15 - 17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match no. Match Type        Team Europe      Score      Team USA  \\\n",
       "0        25    Singles    Mika Koivuniemi  217 - 279  Chris Barnes   \n",
       "1        26    Singles      Osku Palermaa  217 - 244   Tommy Jones   \n",
       "2        27    Singles          Paul Moor  210 - 199      Tim Mack   \n",
       "3        28    Singles     Tore Torgersen  206 - 275     Doug Kent   \n",
       "4        29    Singles  Tomas Leandersson  176 - 258  Bill Hoffman   \n",
       "5        30      Baker        Team Europe  202 - 203      Team USA   \n",
       "6        31    Singles     Tore Torgersen  202 - 264  Chris Barnes   \n",
       "7        32    Singles      Osku Palermaa  196 - 235   Tommy Jones   \n",
       "\n",
       "  Progressive Total  \n",
       "0           14 - 11  \n",
       "1           14 - 12  \n",
       "2           15 - 12  \n",
       "3           15 - 13  \n",
       "4           15 - 14  \n",
       "5           15 - 15  \n",
       "6           15 - 16  \n",
       "7           15 - 17  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in dataset['test']:\n",
    "    table_name = sample['table']['name']\n",
    "    if table_name == 'csv/203-csv/380.tsv':\n",
    "        print(sample['question'])\n",
    "\n",
    "print(dataselect_list[5][0])\n",
    "dataselect_list[5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0be1822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many deaths occurred per year on average in 1975-1980?\n",
      "which period had the highest imr?\n",
      "csv/202-csv/154.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Live births per year</th>\n",
       "      <th>Deaths per year</th>\n",
       "      <th>Natural change per year</th>\n",
       "      <th>CBR1</th>\n",
       "      <th>CDR1</th>\n",
       "      <th>NC1</th>\n",
       "      <th>TFR1</th>\n",
       "      <th>IMR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950-1955</td>\n",
       "      <td>9 000</td>\n",
       "      <td>5 000</td>\n",
       "      <td>4 000</td>\n",
       "      <td>47.9</td>\n",
       "      <td>27.1</td>\n",
       "      <td>20.8</td>\n",
       "      <td>6.67</td>\n",
       "      <td>184.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1955-1960</td>\n",
       "      <td>10 000</td>\n",
       "      <td>6 000</td>\n",
       "      <td>5 000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>26.8</td>\n",
       "      <td>22.3</td>\n",
       "      <td>6.67</td>\n",
       "      <td>181.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1960-1965</td>\n",
       "      <td>12 000</td>\n",
       "      <td>6 000</td>\n",
       "      <td>6 000</td>\n",
       "      <td>48.5</td>\n",
       "      <td>25.7</td>\n",
       "      <td>22.8</td>\n",
       "      <td>6.67</td>\n",
       "      <td>174.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1965-1970</td>\n",
       "      <td>13 000</td>\n",
       "      <td>7 000</td>\n",
       "      <td>7 000</td>\n",
       "      <td>47.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>23.8</td>\n",
       "      <td>6.67</td>\n",
       "      <td>163.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-1975</td>\n",
       "      <td>16 000</td>\n",
       "      <td>7 000</td>\n",
       "      <td>8 000</td>\n",
       "      <td>47.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>25.1</td>\n",
       "      <td>6.67</td>\n",
       "      <td>149.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1975-1980</td>\n",
       "      <td>18 000</td>\n",
       "      <td>8 000</td>\n",
       "      <td>10 000</td>\n",
       "      <td>45.8</td>\n",
       "      <td>19.6</td>\n",
       "      <td>26.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>133.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1980-1985</td>\n",
       "      <td>20 000</td>\n",
       "      <td>8 000</td>\n",
       "      <td>12 000</td>\n",
       "      <td>42.7</td>\n",
       "      <td>17.1</td>\n",
       "      <td>25.6</td>\n",
       "      <td>6.39</td>\n",
       "      <td>117.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1985-1990</td>\n",
       "      <td>21 000</td>\n",
       "      <td>8 000</td>\n",
       "      <td>13 000</td>\n",
       "      <td>40.4</td>\n",
       "      <td>15.0</td>\n",
       "      <td>25.3</td>\n",
       "      <td>6.11</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1990-1995</td>\n",
       "      <td>19 000</td>\n",
       "      <td>7 000</td>\n",
       "      <td>12 000</td>\n",
       "      <td>35.2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>22.7</td>\n",
       "      <td>5.27</td>\n",
       "      <td>87.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1995-2000</td>\n",
       "      <td>16 000</td>\n",
       "      <td>5 000</td>\n",
       "      <td>11 000</td>\n",
       "      <td>29.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>19.3</td>\n",
       "      <td>4.13</td>\n",
       "      <td>69.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-2005</td>\n",
       "      <td>15 000</td>\n",
       "      <td>5 000</td>\n",
       "      <td>11 000</td>\n",
       "      <td>25.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>17.2</td>\n",
       "      <td>3.30</td>\n",
       "      <td>52.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2005-2010</td>\n",
       "      <td>15 000</td>\n",
       "      <td>5 000</td>\n",
       "      <td>10 000</td>\n",
       "      <td>21.5</td>\n",
       "      <td>7.2</td>\n",
       "      <td>14.4</td>\n",
       "      <td>2.61</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Period Live births per year Deaths per year Natural change per year  \\\n",
       "0   1950-1955                9 000           5 000                   4 000   \n",
       "1   1955-1960               10 000           6 000                   5 000   \n",
       "2   1960-1965               12 000           6 000                   6 000   \n",
       "3   1965-1970               13 000           7 000                   7 000   \n",
       "4   1970-1975               16 000           7 000                   8 000   \n",
       "5   1975-1980               18 000           8 000                  10 000   \n",
       "6   1980-1985               20 000           8 000                  12 000   \n",
       "7   1985-1990               21 000           8 000                  13 000   \n",
       "8   1990-1995               19 000           7 000                  12 000   \n",
       "9   1995-2000               16 000           5 000                  11 000   \n",
       "10  2000-2005               15 000           5 000                  11 000   \n",
       "11  2005-2010               15 000           5 000                  10 000   \n",
       "\n",
       "    CBR1  CDR1   NC1  TFR1   IMR1  \n",
       "0   47.9  27.1  20.8  6.67  184.8  \n",
       "1   49.0  26.8  22.3  6.67  181.4  \n",
       "2   48.5  25.7  22.8  6.67  174.1  \n",
       "3   47.8  24.1  23.8  6.67  163.1  \n",
       "4   47.0  22.0  25.1  6.67  149.3  \n",
       "5   45.8  19.6  26.2  6.67  133.2  \n",
       "6   42.7  17.1  25.6  6.39  117.1  \n",
       "7   40.4  15.0  25.3  6.11  104.0  \n",
       "8   35.2  12.5  22.7  5.27   87.5  \n",
       "9   29.2   9.9  19.3  4.13   69.7  \n",
       "10  25.2   7.9  17.2  3.30   52.8  \n",
       "11  21.5   7.2  14.4  2.61   44.4  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in dataset['test']:\n",
    "    table_name = sample['table']['name']\n",
    "    if table_name == 'csv/202-csv/154.tsv':\n",
    "        print(sample['question'])\n",
    "\n",
    "print(dataselect_list[6][0])\n",
    "dataselect_list[6][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f2d1c274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many number of conferences had 2 bids?\n",
      "how many teams played in a championship game?\n",
      "what is the most times a team went to the elite eight?\n",
      "which conference has the most number of bids?\n",
      "what were the number of bids in the sun belt conference?\n",
      "which team is the only team to have a record number of wins in the double digits?\n",
      "how many teams wont at least 5 times?\n",
      "which conference has 2 bids, but a record of 5-2?\n",
      "number of final four appearances by big ten teams?\n",
      "which conferences have had less than 2 bids.\n",
      "which team had the top win % out of the group?\n",
      "what were the total number of times the southeastern conference went to the sweet sixteen?\n",
      "which conference has had at least 6 bids?\n",
      "csv/203-csv/605.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Conference</th>\n",
       "      <th># of Bids</th>\n",
       "      <th>Record</th>\n",
       "      <th>Win %</th>\n",
       "      <th>Round\\nof 32</th>\n",
       "      <th>Sweet\\nSixteen</th>\n",
       "      <th>Elite\\nEight</th>\n",
       "      <th>Final\\nFour</th>\n",
       "      <th>Championship\\nGame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Southeastern</td>\n",
       "      <td>6</td>\n",
       "      <td>10–6</td>\n",
       "      <td>.625</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big Ten</td>\n",
       "      <td>5</td>\n",
       "      <td>9–5</td>\n",
       "      <td>.643</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pacific-10</td>\n",
       "      <td>5</td>\n",
       "      <td>8–5</td>\n",
       "      <td>.615</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southwest</td>\n",
       "      <td>4</td>\n",
       "      <td>5–4</td>\n",
       "      <td>.556</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big Eight</td>\n",
       "      <td>4</td>\n",
       "      <td>3–4</td>\n",
       "      <td>.429</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Atlantic Coast</td>\n",
       "      <td>3</td>\n",
       "      <td>9–2</td>\n",
       "      <td>.818</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Atlantic 10</td>\n",
       "      <td>3</td>\n",
       "      <td>1–3</td>\n",
       "      <td>.250</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sun Belt</td>\n",
       "      <td>2</td>\n",
       "      <td>6–2</td>\n",
       "      <td>.750</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Big East</td>\n",
       "      <td>2</td>\n",
       "      <td>5–2</td>\n",
       "      <td>.714</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Metro</td>\n",
       "      <td>2</td>\n",
       "      <td>2–2</td>\n",
       "      <td>.500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Missouri Valley</td>\n",
       "      <td>2</td>\n",
       "      <td>2–2</td>\n",
       "      <td>.500</td>\n",
       "      <td>2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Big Sky</td>\n",
       "      <td>2</td>\n",
       "      <td>1–2</td>\n",
       "      <td>.333</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Big West</td>\n",
       "      <td>2</td>\n",
       "      <td>0–2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Great Midwest</td>\n",
       "      <td>2</td>\n",
       "      <td>0–2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mid-Continent</td>\n",
       "      <td>2</td>\n",
       "      <td>0–2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>West Coast</td>\n",
       "      <td>2</td>\n",
       "      <td>0–2</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Colonial</td>\n",
       "      <td>1</td>\n",
       "      <td>1–1</td>\n",
       "      <td>.500</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Western Athletic</td>\n",
       "      <td>1</td>\n",
       "      <td>1–1</td>\n",
       "      <td>.500</td>\n",
       "      <td>1</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "      <td>–</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Conference # of Bids Record Win % Round\\nof 32 Sweet\\nSixteen  \\\n",
       "0       Southeastern         6   10–6  .625            5              3   \n",
       "1            Big Ten         5    9–5  .643            4              2   \n",
       "2         Pacific-10         5    8–5  .615            4              2   \n",
       "3          Southwest         4    5–4  .556            3              2   \n",
       "4          Big Eight         4    3–4  .429            2              1   \n",
       "5     Atlantic Coast         3    9–2  .818            3              2   \n",
       "6        Atlantic 10         3    1–3  .250            1              –   \n",
       "7           Sun Belt         2    6–2  .750            2              1   \n",
       "8           Big East         2    5–2  .714            2              2   \n",
       "9              Metro         2    2–2  .500            1              1   \n",
       "10   Missouri Valley         2    2–2  .500            2              –   \n",
       "11           Big Sky         2    1–2  .333            1              –   \n",
       "12          Big West         2    0–2     –            –              –   \n",
       "13     Great Midwest         2    0–2     –            –              –   \n",
       "14     Mid-Continent         2    0–2     –            –              –   \n",
       "15        West Coast         2    0–2     –            –              –   \n",
       "16          Colonial         1    1–1  .500            1              –   \n",
       "17  Western Athletic         1    1–1  .500            1              –   \n",
       "\n",
       "   Elite\\nEight Final\\nFour Championship\\nGame  \n",
       "0             1           1                  –  \n",
       "1             2           1                  –  \n",
       "2             2           –                  –  \n",
       "3             –           –                  –  \n",
       "4             –           –                  –  \n",
       "5             1           1                  1  \n",
       "6             –           –                  –  \n",
       "7             1           1                  1  \n",
       "8             1           –                  –  \n",
       "9             –           –                  –  \n",
       "10            –           –                  –  \n",
       "11            –           –                  –  \n",
       "12            –           –                  –  \n",
       "13            –           –                  –  \n",
       "14            –           –                  –  \n",
       "15            –           –                  –  \n",
       "16            –           –                  –  \n",
       "17            –           –                  –  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in dataset['test']:\n",
    "    table_name = sample['table']['name']\n",
    "    if table_name == 'csv/203-csv/605.tsv':\n",
    "        print(sample['question'])\n",
    "\n",
    "print(dataselect_list[7][0])\n",
    "dataselect_list[7][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b82a5607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the only hospital to have 6 hospital beds?\n",
      "what is the total number of hospital beds at chatham hospital?\n",
      "which hospital has more beds: duke or wake forest?\n",
      "what is the only hospital in burlington?\n",
      "what are the total number of hospital beds in alexander hospital?\n",
      "which city has the most hospitals?\n",
      "what is the last novant affiliated hospital on the table?\n",
      "how many hospitals have at least 10 operating rooms?\n",
      "which hospital has the largest amount of hospital beds?\n",
      "how many rooms total are at cone health?\n",
      "how many hospital have no operating rooms?\n",
      "csv/203-csv/319.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>City</th>\n",
       "      <th>Hospital beds</th>\n",
       "      <th>Operating rooms</th>\n",
       "      <th>Total</th>\n",
       "      <th>Trauma designation</th>\n",
       "      <th>Affiliation</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alamance Regional Medical Center</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>238</td>\n",
       "      <td>15</td>\n",
       "      <td>253</td>\n",
       "      <td>-</td>\n",
       "      <td>Cone</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albemarle Hospital</td>\n",
       "      <td>Elizabeth City</td>\n",
       "      <td>182</td>\n",
       "      <td>13</td>\n",
       "      <td>195</td>\n",
       "      <td>-</td>\n",
       "      <td>Vidant</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander Hospital</td>\n",
       "      <td>Hickory</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alleghany Memorial Hospital</td>\n",
       "      <td>Sparta</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>QHR</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angel Medical Center</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "      <td>-</td>\n",
       "      <td>Mission</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Washington County Hospital</td>\n",
       "      <td>Plymouth</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Watauga Medical Center</td>\n",
       "      <td>Boone</td>\n",
       "      <td>117</td>\n",
       "      <td>9</td>\n",
       "      <td>126</td>\n",
       "      <td>-</td>\n",
       "      <td>ARHS</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Wayne Memorial Hospital</td>\n",
       "      <td>Goldsboro</td>\n",
       "      <td>316</td>\n",
       "      <td>15</td>\n",
       "      <td>331</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Wilkes Regional Medical Center</td>\n",
       "      <td>North Wilkesboro</td>\n",
       "      <td>144</td>\n",
       "      <td>9</td>\n",
       "      <td>153</td>\n",
       "      <td>-</td>\n",
       "      <td>CHS</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Wilson Medical Center</td>\n",
       "      <td>Wilson</td>\n",
       "      <td>294</td>\n",
       "      <td>15</td>\n",
       "      <td>309</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Name              City Hospital beds  \\\n",
       "0    Alamance Regional Medical Center        Burlington           238   \n",
       "1                  Albemarle Hospital    Elizabeth City           182   \n",
       "2                  Alexander Hospital           Hickory            25   \n",
       "3         Alleghany Memorial Hospital            Sparta            41   \n",
       "4                Angel Medical Center          Franklin            59   \n",
       "..                                ...               ...           ...   \n",
       "121        Washington County Hospital          Plymouth            49   \n",
       "122            Watauga Medical Center             Boone           117   \n",
       "123           Wayne Memorial Hospital         Goldsboro           316   \n",
       "124    Wilkes Regional Medical Center  North Wilkesboro           144   \n",
       "125             Wilson Medical Center            Wilson           294   \n",
       "\n",
       "    Operating rooms Total Trauma designation Affiliation Notes  \n",
       "0                15   253                  -        Cone     -  \n",
       "1                13   195                  -      Vidant     -  \n",
       "2                 3    28                  -           -     -  \n",
       "3                 2    43                  -         QHR     -  \n",
       "4                 7    66                  -     Mission     -  \n",
       "..              ...   ...                ...         ...   ...  \n",
       "121               2    51                  -           -     -  \n",
       "122               9   126                  -        ARHS     -  \n",
       "123              15   331                  -           -     -  \n",
       "124               9   153                  -         CHS     -  \n",
       "125              15   309                  -           -     -  \n",
       "\n",
       "[126 rows x 8 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in dataset['test']:\n",
    "    table_name = sample['table']['name']\n",
    "    if table_name == 'csv/203-csv/319.tsv':\n",
    "        print(sample['question'])\n",
    "\n",
    "print(dataselect_list[8][0])\n",
    "dataselect_list[8][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ba1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates for time related questions\n",
    "\n",
    "weekdays1 = \"On which day of the week did <event> happen?\"\n",
    "weekdays2 = \"Was <event> on the weekend?\"\n",
    "weekdasy3 = \"How many <entity> where <event> on <weekday>?\"\n",
    "datediff = \"How many <time_unit> lie between <event> and <event>?\"\n",
    "cycle = \"How many <event> occured when there was <cycle>?\" # cycle [a full moon, a leap year, even day, spring] \n",
    "coocurence = \"Event happened on <datetime> how many <event> happend at the same time?\"\n",
    "offset = \"How many <event> happened <time difference> <event>?\"\n",
    "\n",
    "# Templates for numeric understanding\n",
    "\n",
    "within_range = \"How many <event> that are <comparison_op> <quntification> <event>?\"\n",
    "percentile = \"Which <entity_header> are among the <percentile> in terms of <score_entity>?\"\n",
    "percentile \"What is the average <score_entity> of the <percentile> <entity_header>?\"\n",
    "outlier = \"Which are the <entity_header> have the highest distance in <score_entity> to others?\"\n",
    "outlier2 = \"Which <entity_header> have unusual <score_entity> but common <score_entity>?\" # difine by histogram with x bars where smaller or greater mean bar hight\n",
    "comparisons = \"How many <entity_header> have <quantifier> <score_entity> than <entity>?\" # or simple statement verification \"1.2232 is higher than 1.22291112?\" then test train/test different value ranges\n",
    "\n",
    "\n",
    "# + chaining conditions\n",
    "\n",
    "# multidimensional (alphanumeric) columns\n",
    "# 2-3-4, x 45 y 34, 193.131.23432\n",
    "\n",
    "# unit_conversion 103cm > 1.3m\n",
    "# test alignment of fahrenheit vs celsius column with same row entries in text vs numeric/distribution encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accbe3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    TapexTokenizer\n",
    ")\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from filelock import FileLock\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "        \n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Pretrained tokenizer name or path if not the same as model_name. \"\n",
    "                \"By default we use BART-large tokenizer for TAPEX-large.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14964ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: the initial BART model's decoding is penalized by no_repeat_ngram_size, and thus\n",
    "    # we should disable it here to avoid problematic generation\n",
    "    config.no_repeat_ngram_size = 0\n",
    "    config.max_length = 1024\n",
    "    config.early_stopping = False\n",
    "\n",
    "    # load tapex tokenizer\n",
    "    tokenizer = TapexTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "\n",
    "    # load Bart based Tapex model (default tapex-large)\n",
    "    model = BartForConditionalGeneration.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4f0374e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m  \u001b[38;5;66;03m# Here to have a nice missing dependency error message early on\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2022 The Microsoft and The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    "Fine-tuning the library models for tapex on table-based question answering tasks.\n",
    "Adapted from script: https://github.com/huggingface/transformers/blob/master/examples/pytorch/summarization/run_summarization.py\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "from typing import List, Optional\n",
    "\n",
    "import nltk  # Here to have a nice missing dependency error message early on\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from filelock import FileLock\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    BartForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TapexTokenizer,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.17.0.dev0\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Pretrained tokenizer name or path if not the same as model_name. \"\n",
    "                \"By default we use BART-large tokenizer for TAPEX-large.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `huggingface-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=\"wikitablequestions\", metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    max_source_length: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_target_length: Optional[int] = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    val_max_target_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n",
    "                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`.\"\n",
    "                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n",
    "                \"during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Whether to pad all samples to model maximum sentence length. \"\n",
    "                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n",
    "                \"efficient on GPU but very bad for TPU.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    num_beams: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n",
    "                \"which is used during ``evaluate`` and ``predict``.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    ignore_pad_token_for_loss: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n",
    "        if self.val_max_target_length is None:\n",
    "            self.val_max_target_length = self.max_target_length\n",
    "\n",
    "\n",
    "def main():\n",
    "    # See all possible arguments in src/transformers/training_args.py\n",
    "    # or by passing the --help flag to this script.\n",
    "    # We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Detecting last checkpoint.\n",
    "    last_checkpoint = None\n",
    "    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "            raise ValueError(\n",
    "                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "                \"Use --overwrite_output_dir to overcome.\"\n",
    "            )\n",
    "        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "            logger.info(\n",
    "                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "            )\n",
    "\n",
    "    # Setup logging\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n",
    "\n",
    "    # Log on each process the small summary:\n",
    "    logger.warning(\n",
    "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
    "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
    "    )\n",
    "    # Set the verbosity to info of the Transformers logger (on main process only):\n",
    "    if is_main_process(training_args.local_rank):\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "    # Set seed before initializing model.\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n",
    "    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "    # (the dataset will be downloaded automatically from the datasets Hub).\n",
    "    #\n",
    "    # For JSON files, this script will use the `question` column for the input question and `table` column for the corresponding table.\n",
    "    #\n",
    "    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if data_args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        datasets = load_dataset(data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir)\n",
    "    else:\n",
    "        data_files = {}\n",
    "        if data_args.train_file is not None:\n",
    "            data_files[\"train\"] = data_args.train_file\n",
    "            extension = data_args.train_file.split(\".\")[-1]\n",
    "        if data_args.validation_file is not None:\n",
    "            data_files[\"validation\"] = data_args.validation_file\n",
    "            extension = data_args.validation_file.split(\".\")[-1]\n",
    "        if data_args.test_file is not None:\n",
    "            data_files[\"test\"] = data_args.test_file\n",
    "            extension = data_args.test_file.split(\".\")[-1]\n",
    "        datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)\n",
    "\n",
    "    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "    # https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    #\n",
    "    # Distributed training:\n",
    "    # The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "    # download model & vocab.\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: the initial BART model's decoding is penalized by no_repeat_ngram_size, and thus\n",
    "    # we should disable it here to avoid problematic generation\n",
    "    config.no_repeat_ngram_size = 0\n",
    "    config.max_length = 1024\n",
    "    config.early_stopping = False\n",
    "\n",
    "    # load tapex tokenizer\n",
    "    tokenizer = TapexTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        use_fast=model_args.use_fast_tokenizer,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "        add_prefix_space=True,\n",
    "    )\n",
    "\n",
    "    # load Bart based Tapex model (default tapex-large)\n",
    "    model = BartForConditionalGeneration.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        revision=model_args.model_revision,\n",
    "        use_auth_token=True if model_args.use_auth_token else None,\n",
    "    )\n",
    "\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize inputs and targets.\n",
    "    if training_args.do_train:\n",
    "        column_names = datasets[\"train\"].column_names\n",
    "    elif training_args.do_eval:\n",
    "        column_names = datasets[\"validation\"].column_names\n",
    "    elif training_args.do_predict:\n",
    "        column_names = datasets[\"test\"].column_names\n",
    "    else:\n",
    "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
    "        return\n",
    "\n",
    "    # Temporarily set max_target_length for training.\n",
    "    max_target_length = data_args.max_target_length\n",
    "    padding = \"max_length\" if data_args.pad_to_max_length else False\n",
    "\n",
    "    if training_args.label_smoothing_factor > 0 and not hasattr(model, \"prepare_decoder_input_ids_from_labels\"):\n",
    "        logger.warning(\n",
    "            \"label_smoothing is enabled but the `prepare_decoder_input_ids_from_labels` method is not defined for\"\n",
    "            f\"`{model.__class__.__name__}`. This will lead to loss being calculated twice and will take up more memory\"\n",
    "        )\n",
    "\n",
    "    def preprocess_tableqa_function(examples, is_training=False):\n",
    "        \"\"\"\n",
    "        The is_training FLAG is used to identify if we could use the supervision\n",
    "        to truncate the table content if it is required.\n",
    "        \"\"\"\n",
    "\n",
    "        questions = [question.lower() for question in examples[\"question\"]]\n",
    "        example_tables = examples[\"table\"]\n",
    "        tables = [\n",
    "            pd.DataFrame.from_records(example_table[\"rows\"], columns=example_table[\"header\"])\n",
    "            for example_table in example_tables\n",
    "        ]\n",
    "\n",
    "        # using wikitablequestion's answer set\n",
    "        answers = examples[\"answers\"]\n",
    "\n",
    "        # IMPORTANT: we cannot pass by answers during evaluation, answers passed during training are used to\n",
    "        # truncate large tables in the train set!\n",
    "        if is_training:\n",
    "            model_inputs = tokenizer(\n",
    "                table=tables,\n",
    "                query=questions,\n",
    "                answer=answers,\n",
    "                max_length=data_args.max_source_length,\n",
    "                padding=padding,\n",
    "                truncation=True,\n",
    "            )\n",
    "        else:\n",
    "            model_inputs = tokenizer(\n",
    "                table=tables, query=questions, max_length=data_args.max_source_length, padding=padding, truncation=True\n",
    "            )\n",
    "\n",
    "        labels = tokenizer(\n",
    "            answer=[\", \".join(answer) for answer in answers],\n",
    "            max_length=max_target_length,\n",
    "            padding=padding,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n",
    "            labels[\"input_ids\"] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "            ]\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    # in training, we can use the answer as extra information to truncate large tables\n",
    "    preprocess_tableqa_function_training = partial(preprocess_tableqa_function, is_training=True)\n",
    "\n",
    "    if training_args.do_train:\n",
    "        if \"train\" not in datasets:\n",
    "            raise ValueError(\"--do_train requires a train dataset\")\n",
    "        train_dataset = datasets[\"train\"]\n",
    "        if data_args.max_train_samples is not None:\n",
    "            train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "        train_dataset = train_dataset.map(\n",
    "            preprocess_tableqa_function_training,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    if training_args.do_eval:\n",
    "        max_target_length = data_args.val_max_target_length\n",
    "        if \"validation\" not in datasets:\n",
    "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "        eval_dataset = datasets[\"validation\"]\n",
    "        if data_args.max_eval_samples is not None:\n",
    "            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
    "        eval_dataset = eval_dataset.map(\n",
    "            preprocess_tableqa_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        max_target_length = data_args.val_max_target_length\n",
    "        if \"test\" not in datasets:\n",
    "            raise ValueError(\"--do_predict requires a test dataset\")\n",
    "        predict_dataset = datasets[\"test\"]\n",
    "        if data_args.max_predict_samples is not None:\n",
    "            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n",
    "        predict_dataset = predict_dataset.map(\n",
    "            preprocess_tableqa_function,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "        )\n",
    "\n",
    "    # Data collator\n",
    "    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8 if training_args.fp16 else None,\n",
    "    )\n",
    "\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        if data_args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        delimiter = \", \"\n",
    "\n",
    "        # define example evaluation\n",
    "        def evaluate_example(predict_str: str, ground_str: str):\n",
    "            predict_spans = predict_str.split(delimiter)\n",
    "            ground_spans = ground_str.split(delimiter)\n",
    "            predict_values = defaultdict(lambda: 0)\n",
    "            ground_values = defaultdict(lambda: 0)\n",
    "            for span in predict_spans:\n",
    "                try:\n",
    "                    predict_values[float(span)] += 1\n",
    "                except ValueError:\n",
    "                    predict_values[span.strip()] += 1\n",
    "            for span in ground_spans:\n",
    "                try:\n",
    "                    ground_values[float(span)] += 1\n",
    "                except ValueError:\n",
    "                    ground_values[span.strip()] += 1\n",
    "            _is_correct = predict_values == ground_values\n",
    "            return _is_correct\n",
    "\n",
    "        def get_denotation_accuracy(predictions: List[str], references: List[str]):\n",
    "            assert len(predictions) == len(references)\n",
    "            correct_num = 0\n",
    "            for predict_str, ground_str in zip(predictions, references):\n",
    "                is_correct = evaluate_example(predict_str.lower(), ground_str.lower())\n",
    "                if is_correct:\n",
    "                    correct_num += 1\n",
    "            return correct_num / len(predictions)\n",
    "\n",
    "        accuracy = get_denotation_accuracy(decoded_preds, decoded_labels)\n",
    "        result = {\"denotation_accuracy\": accuracy}\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset if training_args.do_train else None,\n",
    "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
    "    )\n",
    "\n",
    "    if training_args.do_train:\n",
    "        checkpoint = None\n",
    "        if training_args.resume_from_checkpoint is not None:\n",
    "            checkpoint = training_args.resume_from_checkpoint\n",
    "        elif last_checkpoint is not None:\n",
    "            checkpoint = last_checkpoint\n",
    "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "        trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "        metrics = train_result.metrics\n",
    "        max_train_samples = (\n",
    "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
    "        )\n",
    "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "        trainer.save_metrics(\"train\", metrics)\n",
    "        trainer.save_state()\n",
    "\n",
    "    # Evaluation\n",
    "    results = {}\n",
    "    if training_args.do_eval:\n",
    "        logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "        metrics = trainer.evaluate(\n",
    "            max_length=data_args.val_max_target_length, num_beams=data_args.num_beams, metric_key_prefix=\"eval\"\n",
    "        )\n",
    "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
    "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"eval\", metrics)\n",
    "        trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "    if training_args.do_predict:\n",
    "        logger.info(\"*** Predict ***\")\n",
    "\n",
    "        predict_results = trainer.predict(\n",
    "            predict_dataset,\n",
    "            metric_key_prefix=\"predict\",\n",
    "            max_length=data_args.val_max_target_length,\n",
    "            num_beams=data_args.num_beams,\n",
    "        )\n",
    "        metrics = predict_results.metrics\n",
    "        max_predict_samples = (\n",
    "            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
    "        )\n",
    "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
    "\n",
    "        trainer.log_metrics(\"predict\", metrics)\n",
    "        trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "        if trainer.is_world_process_zero():\n",
    "            if training_args.predict_with_generate:\n",
    "                predictions = tokenizer.batch_decode(\n",
    "                    predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "                )\n",
    "                predictions = [pred.strip() for pred in predictions]\n",
    "                output_prediction_file = os.path.join(training_args.output_dir, \"tapex_predictions.txt\")\n",
    "                with open(output_prediction_file, \"w\") as writer:\n",
    "                    writer.write(\"\\n\".join(predictions))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _mp_fn(index):\n",
    "    # For xla_spawn (TPUs)\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fdf0e39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'mmap' is an invalid keyword argument for Unpickler()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_torch_save.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(list_e, file_name)\n\u001b[0;32m----> 7\u001b[0m loaded_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(file_name, mmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m loaded_list\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/TabEmbed/lib/python3.11/site-packages/torch/serialization.py:1170\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;66;03m# Load the data (which may in turn use `persistent_load` to load tensors)\u001b[39;00m\n\u001b[1;32m   1168\u001b[0m data_file \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(zip_file\u001b[38;5;241m.\u001b[39mget_record(pickle_file))\n\u001b[0;32m-> 1170\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m   1172\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'mmap' is an invalid keyword argument for Unpickler()"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "list_e = [1,2]\n",
    "file_name = 'test_torch_save.pkl'\n",
    "\n",
    "torch.save(list_e, file_name)\n",
    "\n",
    "loaded_list = torch.load(file_name, mmap=True)\n",
    "loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a9c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
